{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TGINE/blob/main/2_1_Gate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_-ImClQ1_Ot"
      },
      "source": [
        "# Sesión 2 - Python GATEnlp\n",
        "\n",
        "Python GATEnlp es una herramienta gráfica que permite definir distintos pipelines para la obtención de anotaciones a través del uso de Gazzetteers y reglas.\n",
        "\n",
        "El objetivo de la práctica es ver las posibilidades de GATE y crear distintos recursos para realizar la detección de entidades usando un enfoque basado en conocimiento. Iremos procesando un texto de ejemplo.\n",
        "\n",
        "Lo primero que haremos será instalar GATEnlp y stanza y descargar el modelo en español de stanza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jQ1tLgBaG1z",
        "outputId": "824c1583-4853-40d3-b339-0728a4098693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gatenlp[all]\n",
            "  Downloading gatenlp-1.0.8-py3-none-any.whl (335 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.6/335.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (2.4.0)\n",
            "Collecting iobes (from gatenlp[all])\n",
            "  Downloading iobes-1.5.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (4.11.2)\n",
            "Collecting conllu (from gatenlp[all])\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting elg (from gatenlp[all])\n",
            "  Downloading elg-0.5.0-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (2.84.0)\n",
            "Requirement already satisfied: google-cloud-language in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (2.9.1)\n",
            "Collecting ibm-watson (from gatenlp[all])\n",
            "  Downloading ibm-watson-7.0.1.tar.gz (389 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (1.0.7)\n",
            "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (3.8.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (0.10.9.7)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (6.0.1)\n",
            "Collecting ray[default] (from gatenlp[all])\n",
            "  Downloading ray-2.8.0-cp310-cp310-manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (2.31.0)\n",
            "Requirement already satisfied: spacy>=2.2 in /usr/local/lib/python3.10/dist-packages (from gatenlp[all]) (3.6.1)\n",
            "Collecting stanza>=1.3.0 (from gatenlp[all])\n",
            "  Downloading stanza-1.6.1-py3-none-any.whl (881 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.2/881.2 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tner (from gatenlp[all])\n",
            "  Downloading tner-0.2.4.tar.gz (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9.3->gatenlp[all]) (2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->gatenlp[all]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->gatenlp[all]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->gatenlp[all]) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->gatenlp[all]) (4.66.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (6.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (1.23.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (3.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gatenlp[all]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gatenlp[all]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gatenlp[all]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gatenlp[all]) (2023.7.22)\n",
            "Collecting emoji (from stanza>=1.3.0->gatenlp[all])\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza>=1.3.0->gatenlp[all]) (3.20.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza>=1.3.0->gatenlp[all]) (2.1.0+cu118)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from elg->gatenlp[all]) (1.5.3)\n",
            "Collecting loguru>=0.5 (from elg->gatenlp[all])\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->gatenlp[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->gatenlp[all]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->gatenlp[all]) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->gatenlp[all]) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->gatenlp[all]) (4.1.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-language->gatenlp[all]) (1.22.3)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from ibm-watson->gatenlp[all]) (2.8.2)\n",
            "Requirement already satisfied: websocket-client>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from ibm-watson->gatenlp[all]) (1.6.4)\n",
            "Collecting ibm-cloud-sdk-core==3.*,>=3.3.6 (from ibm-watson->gatenlp[all])\n",
            "  Downloading ibm-cloud-sdk-core-3.17.3.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->gatenlp[all])\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyJWT<3.0.0,>=2.8.0 (from ibm-cloud-sdk-core==3.*,>=3.3.6->ibm-watson->gatenlp[all])\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (4.19.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (1.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (3.8.6)\n",
            "Collecting aiohttp-cors (from ray[default]->gatenlp[all])\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default]->gatenlp[all])\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default]->gatenlp[all])\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpustat>=1.0.0 (from ray[default]->gatenlp[all])\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencensus (from ray[default]->gatenlp[all])\n",
            "  Downloading opencensus-0.11.3-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (0.18.0)\n",
            "Collecting virtualenv<20.21.1,>=20.0.24 (from ray[default]->gatenlp[all])\n",
            "  Downloading virtualenv-20.21.0-py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]->gatenlp[all]) (1.59.2)\n",
            "Collecting allennlp>=2.0.0 (from tner->gatenlp[all])\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from tner->gatenlp[all])\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from tner->gatenlp[all])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from tner->gatenlp[all])\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets (from tner->gatenlp[all])\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]->gatenlp[all]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]->gatenlp[all]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]->gatenlp[all]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]->gatenlp[all]) (1.9.2)\n",
            "Collecting torch>=1.3.0 (from stanza>=1.3.0->gatenlp[all])\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.14.0,>=0.8.1 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting spacy>=2.2 (from gatenlp[all])\n",
            "  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=1.2 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (1.11.3)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (7.4.3)\n",
            "Collecting transformers (from tner->gatenlp[all])\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from ray[default]->gatenlp[all])\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading huggingface_hub-0.19.0-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.4 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting base58>=2.1.1 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting sacremoses (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp>=2.0.0->tner->gatenlp[all]) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting thinc<8.1.0,>=8.0.14 (from spacy>=2.2->gatenlp[all])\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy>=2.2->gatenlp[all])\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting typer>=0.4.1 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic>=1.7.4 (from elg->gatenlp[all])\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.2->gatenlp[all]) (4.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client->gatenlp[all]) (1.61.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client->gatenlp[all]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->gatenlp[all]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->gatenlp[all]) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->gatenlp[all]) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client->gatenlp[all]) (4.9)\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat>=1.0.0->ray[default]->gatenlp[all])\n",
            "  Downloading nvidia_ml_py-12.535.133-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]->gatenlp[all]) (5.9.5)\n",
            "Collecting blessed>=1.17.1 (from gpustat>=1.0.0->ray[default]->gatenlp[all])\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->gatenlp[all]) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->elg->gatenlp[all]) (2023.3.post1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers->tner->gatenlp[all])\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.6 (from virtualenv<20.21.1,>=20.0.24->ray[default]->gatenlp[all])\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]->gatenlp[all]) (3.11.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tner->gatenlp[all]) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->tner->gatenlp[all]) (3.4.1)\n",
            "Collecting multiprocess (from datasets->tner->gatenlp[all])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->tner->gatenlp[all]) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.2->gatenlp[all]) (2.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]->gatenlp[all]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]->gatenlp[all]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]->gatenlp[all]) (0.11.0)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]->gatenlp[all])\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]->gatenlp[all]) (0.2.9)\n",
            "Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading boto3-1.28.82-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all]) (2.8.0)\n",
            "INFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting cached-path<1.2.0,>=1.1.3 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading cached_path-1.1.5-py3-none-any.whl (26 kB)\n",
            "  Downloading cached_path-1.1.4-py3-none-any.whl (26 kB)\n",
            "  Downloading cached_path-1.1.3-py3-none-any.whl (26 kB)\n",
            "Collecting datasets (from tner->gatenlp[all])\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.1/519.1 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.14.1-py3-none-any.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.4/492.4 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.4 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from tner->gatenlp[all])\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets->tner->gatenlp[all])\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting datasets (from tner->gatenlp[all])\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.16 (from allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client->gatenlp[all]) (0.5.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner->gatenlp[all]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner->gatenlp[all]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner->gatenlp[all]) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp>=2.0.0->tner->gatenlp[all]) (2.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp>=2.0.0->tner->gatenlp[all]) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp>=2.0.0->tner->gatenlp[all]) (9.4.0)\n",
            "Collecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all]) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading sentry_sdk-1.34.0-py2.py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from datasets->tner->gatenlp[all])\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.32.0,>=1.31.82 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading botocore-1.31.82-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all]) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all]) (2.6.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all]) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp>=2.0.0->tner->gatenlp[all])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp>=2.0.0->tner->gatenlp[all]) (1.5.0)\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core, tner, fairscale, termcolor, gpustat, seqeval, jsonnet, pathtools\n",
            "  Building wheel for ibm-watson (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-7.0.1-py3-none-any.whl size=389785 sha256=247972ac6952b50fe925a7b8626e97879cb91077c828d3abf0c16ea8945340d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/df/f4/f8edc5ba0637dd4bfb2029741ae20402976a49d1b6bc113553\n",
            "  Building wheel for ibm-cloud-sdk-core (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.17.3-py3-none-any.whl size=87034 sha256=23616d7db2a881a6616263cb9ebf0fe5ed3f9002c4b03da83fa9f075da6ad46b\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/f8/2a/252525dc6e246fa21beb8f9d287d22dae8bd1036dc9dd9690d\n",
            "  Building wheel for tner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tner: filename=tner-0.2.4-py3-none-any.whl size=39757 sha256=568924f4e6073ab4030aca84dbaaeafb2b0f52e922845a392f309cd008726f5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/57/f6/297e90e242ab8befffe9b1fe05896097c67ba2322a0da606a4\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307221 sha256=f877bdc6841bcee2a42dbc00152dca10037464f7f9f3a075631f422b30ee7bbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=11b3b5ef6b58e7c00d5eedfa4aeb7a1a49538c4d51e68c5ad355a40f2262e9fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26535 sha256=9c22e8a488aec04b5aa7e038593096b7d6d9043b1ba8b1a66137f283d81a8faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=451610d9914fbe53c27c97f063d2bcecf26107c371ad8cf3070590e3fb1673db\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n"
          ]
        }
      ],
      "source": [
        "#quitamos que se muestren los mensajes de log como DEBUG e INFO\n",
        "import logging, sys\n",
        "logging.disable(sys.maxsize)\n",
        "\n",
        "\n",
        "!pip3 install gatenlp[all]\n",
        "\n",
        "!pip3 install stanza\n",
        "import stanza\n",
        "stanza.download('es')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYHxHThPanB9"
      },
      "source": [
        "## Apartado 1.1 (Resuelto)\n",
        "\n",
        "Cargamos un documento en inglés y lo anotamos con el servicio de GateCloudAnnotator (https://cloud.gate.ac.uk/) y más concretamente el de ANNIE. Este componente permite la detección de distintas entidades como son:\n",
        "*   :Person\n",
        "* :Location\n",
        "* :Organization\n",
        "* :Date\n",
        "* :Address\n",
        "* :Money\n",
        "* :Percent\n",
        "* :Token\n",
        "* :SpaceToken\n",
        "* :Sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3oxzxIbamKQ"
      },
      "outputs": [],
      "source": [
        "# Definimos un ejemplo de texto en inglés.\n",
        "texto_en= \"\"\"\n",
        "Roger Federer (German: [ˈrɔdʒər ˈfeːdərər]; born 8 August 1981) is a Swiss professional tennis player. He is ranked No. 9 in the world by the Association of Tennis Professionals (ATP). He has won 20 Grand Slam men's singles titles, an all-time record shared with Rafael Nadal and Novak Djokovic. Federer has been world No. 1 in the ATP rankings a total of 310 weeks – including a record 237 consecutive weeks – and has finished as the year-end No. 1 five times. Federer has won 103 ATP singles titles, the second most of all-time behind Jimmy Connors, including a record six ATP Finals.\n",
        "\n",
        "Federer has played in an era where he dominated men's tennis together with Rafael Nadal and Novak Djokovic, who have been collectively referred to as the Big Three and are widely considered three of the greatest tennis players of all-time.[c] A Wimbledon junior champion in 1998, Federer won his first Grand Slam singles title at Wimbledon in 2003 at age 21. In 2004, he won three out of the four major singles titles and the ATP Finals,[d] a feat he repeated in 2006 and 2007. From 2005 to 2010, Federer made 18 out of 19 major singles finals. During this span, he won his fifth consecutive titles at both Wimbledon and the US Open. He completed the career Grand Slam at the 2009 French Open after three previous runner-ups to Nadal, his main rival up until 2010. At age 27, he also surpassed Pete Sampras's then-record of 14 Grand Slam men's singles titles at Wimbledon in 2009.\n",
        "\n",
        "Although Federer remained in the top 3 through most of the 2010s, the success of Djokovic and Nadal in particular ended his dominance over grass and hard courts. From mid-2010 through the end of 2016, he only won one major title. During this period, Federer and Stan Wawrinka led the Switzerland Davis Cup team to their first title in 2014, adding to the gold medal they won together in doubles at the 2008 Beijing Olympics. Federer also has a silver medal in singles from the 2012 London Olympics, where he finished runner-up to Andy Murray. After taking half a year off in late 2016 to recover from knee surgery, Federer had a renaissance at the majors. He won three more Grand Slam singles titles over the next two years, including the 2017 Australian Open over Nadal and a men's singles record eighth Wimbledon title later in 2017. He also became the oldest ATP world No. 1 in 2018 at age 36.\n",
        "\n",
        "A versatile all-court player, Federer's perceived effortlessness has made him highly popular among tennis fans. Originally lacking self-control as a junior, Federer transformed his on-court demeanor to become well-liked for his general graciousness, winning the Stefan Edberg Sportsmanship Award 13 times. He has also won the Laureus World Sportsman of the Year award a record five times. Outside of competing, he played an instrumental role in the creation of the Laver Cup team competition. Federer is also an active philanthropist. He established the Roger Federer Foundation, which targets impoverished children in southern Africa, and has raised funds in part through the Match for Africa exhibition series. Federer is routinely one of the top ten highest-paid athletes in any sport and ranked first among all athletes with $100 million in endorsement income in 2020.\"\"\"\n",
        "\n",
        "from gatenlp import Document\n",
        "from gatenlp.processing.client.gatecloud import GateCloudAnnotator\n",
        "\n",
        "# Definimos el anotador en la nube\n",
        "annotator = GateCloudAnnotator(\n",
        "    url=\"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\",\n",
        "    outset_name=\"ANNIE\",\n",
        "    ann_types=\":Address,:Date,:Location,:Organization,:Person,:Money,:Percent,:Token,:SpaceToken,:Sentence\"\n",
        ")\n",
        "\n",
        "doc = Document(texto_en)\n",
        "# Ejecutamos el annotador y mostramos el documento anotado\n",
        "doc = annotator(doc)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82rH1prP4xlC"
      },
      "source": [
        "## Apartado 1.2 (Resuelto)\n",
        "\n",
        "Vamos a hacer un ejemplo de reglas para la detección de entidades usando los Gazetteers que son un conjunto de listas de palabras que se identificarán en GATE. Para ello descargamos el ejemplo que se proporciona y está en el AulaVirtual y se descomprime.\n",
        "\n",
        "Cargamos un texto de ejemplo que se proporciona en español y se muestra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQJ-yvFNnfMh"
      },
      "outputs": [],
      "source": [
        "#Descargamos los ficheros de ejemplo\n",
        "!wget -q http://dis.um.es/~valencia/recursosTGINE/gatenlpUM.zip -O gatenlpUM.zip\n",
        "!unzip -o gatenlpUM.zip\n",
        "\n",
        "import os\n",
        "from gatenlp import Document\n",
        "from gatenlp.processing.gazetteer import TokenGazetteer, StringGazetteer\n",
        "from gatenlp.processing.tokenizer import NLTKTokenizer\n",
        "\n",
        "# Cargamos un documento a partir de un fichero y lo mmostramos\n",
        "doc = Document.load(\"rafa_nadal.txt\")\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf0AylVu_ma4"
      },
      "source": [
        "## Apartado 1.3 (Resuelto)\n",
        "\n",
        "Para poder usar los módulos de Gazetteer y PAMPAC de GATE es necesario tener un Tokenizer definido. Nosotros vamos a usar el Tokenizer de Stanza. No solamente utilizaremos el Tokenizer sino que también obtendremos las categorías gramaticales haciendo uso del POS Tagger y también utilizaremos la detección de entidades. Para esto se define una función llamada *obtanerAnotacionesStanzaEnGate* que se describe a continuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQp-tkQAkiMw"
      },
      "outputs": [],
      "source": [
        "# definimos una función para crear las anotaciones de Stanza para que se muestren en GATE\n",
        "def obtenerAnotacionesStanzaEnGate (doc):\n",
        "  import string\n",
        "  spanish_punctuation = string.punctuation + '¿'+'¡'\n",
        "\n",
        "  nlp = stanza.Pipeline(lang='es', processors='tokenize,pos,ner')\n",
        "  doctext = nlp(doc.text)\n",
        "\n",
        "  annset = doc.annset()\n",
        "  for sent in doctext.sentences:\n",
        "    for tok in sent.tokens:\n",
        "      kind = \"word\"\n",
        "      orth = \"lowercase\"\n",
        "      if tok.text.isupper():\n",
        "        orth = \"uppercase\"\n",
        "      elif tok.text[0].isupper():\n",
        "        orth = \"upperInitial\"\n",
        "      if tok.text.isnumeric():\n",
        "        kind = \"number\"\n",
        "      elif tok.text in spanish_punctuation:\n",
        "        kind = \"punctuation\"\n",
        "      ann = annset.add(tok.start_char,tok.end_char,\"Token\",{'string':tok.text, 'kind':kind, 'orth':orth, 'length':tok.end_char-tok.start_char, 'pos': tok.words[0].upos if tok.words[0].upos else \"\"})\n",
        "  for ent in doctext.ents:\n",
        "      ann = annset.add(ent.start_char,ent.end_char, ent.type,{'string': ent.text})\n",
        "  return doc\n",
        "\n",
        "#limpiamos las anotaciones\n",
        "doc.annset().clear()\n",
        "#ejecutamos la función de anotación de Stanza\n",
        "doc=obtenerAnotacionesStanzaEnGate(doc)\n",
        "#mostramos el documento\n",
        "doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWzLNQe6ALFI"
      },
      "source": [
        "## Apartado 1.4 - Gazetteer (Resuelto)\n",
        "\n",
        "Los Gazetteers son listas de expresiones de texto que representan algo como pueden ser nombres de ciudades, nombres de primera persona, meses del año, etc. Todas estas listas se definen en fichero list.def. Este fichero es un índice con el siguiente formato:\n",
        "loc_spanish_city.lst:location:city\n",
        "spanish_firstname.lst:person_first\n",
        "\n",
        "```\n",
        "loc_spanish_city.lst:location:city\n",
        "spanish_firstname.lst:person_first\n",
        "```\n",
        "En la primera columna se define el nombre del fichero que contiene la lista y seguidamente se define el **majorType** y el **minorType**.\n",
        "\n",
        "Las anotaciones resultantes del proceso se suelen guardar en el tipo de anotación **Lookup**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0_zFx0fbTtx"
      },
      "outputs": [],
      "source": [
        "#limpiamos las anotaciones\n",
        "doc.annset().clear()\n",
        "\n",
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO61qWWXF2j0"
      },
      "source": [
        "## Apartado 1.5 (Resuelto)\n",
        "\n",
        "PAMPAC “PAttern Matching with PArser Combinators” permite definir reglas complejas para la anotación de entidades en el texto a partir de patrones de texto.\n",
        "\n",
        "Para eso se definen un conjunto de reglas que se basan en un tipo de expresiones regulares. Por ejemplo, la siguiente regla obtendrá todas las anotaciones de tipo **Lookup** cuyo majorType sea *\"location\"* y creará una nueva anotación llamada **LOC**.\n",
        "\n",
        "```\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNkAJ2ZqeQrv"
      },
      "outputs": [],
      "source": [
        "from gatenlp.pam.pampac import Ann, AnnAt, Rule, Pampac, AddAnn, N, Seq, Or\n",
        "from gatenlp.pam.matcher import FeatureMatcher\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFXG6Tr0KWEP"
      },
      "source": [
        "## Apartado 1.6\n",
        "Vamos a crear un conjunto de reglas para identificar nombres de persona en español:\n",
        "* Creamos un nuevo gazetteer “surname.lst”.\n",
        "* Insertamos “Nadal”, \"Parera\", \"Djokovic\" y “Ferrer” en la lista.\n",
        "* Insertamos una nueva línea en el fichero list.def:\n",
        " * surname.lst:surname\n",
        "\n",
        "Creamos nuevas reglas para identificar personas (**PER**):\n",
        "* Regla 2:\n",
        "  * Una persona se forma por un “person_first” y un “surname”\n",
        "\n",
        "```\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "      AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "      AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "      name=\"person1\"\n",
        "      ),\n",
        "    # then the action for the patter\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "```\n",
        "\n",
        "* Regla 3:\n",
        "  * Una persona se forma por un “person_first” y un *Token* con su primer caracter en *uppercase*\n",
        "\n",
        "* Regla 4:\n",
        "  * Una persona se forma por un *Token* con su primer caracter en *uppercase* y un  *“surname”*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2W3zmdJHVl-"
      },
      "outputs": [],
      "source": [
        "#limpiamos las anotaciones\n",
        "doc.annset().clear()\n",
        "\n",
        "doc=obtenerAnotacionesStanzaEnGate(doc)\n",
        "\n",
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "        name=\"person1\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r4\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2, r3, r4]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7QlZq2iXDvL"
      },
      "source": [
        "## Apartado 1.7\n",
        "Modificamos la regla 3 para indicar que una **PER** está formada por 1 o 2 *“person_first”*\n",
        "\n",
        "```\n",
        "Rule 3:\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "```\n",
        "\n",
        "Modificamos la regla 4 indicando que una **PER** está formada por 1 o 2 *“surname”*\n",
        "\n",
        "\n",
        "```\n",
        "Rule 4:\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyOVO1WFXSIb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(N(AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),min=1, max=2), N(AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),min=1,max=2), name=\"person1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r4\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER\", features=dict(rule=\"person3\"))\n",
        "    )\n",
        "\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2, r3, r4]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBilOcY9Ye0C"
      },
      "source": [
        "## Ejercicio 1\n",
        "Crear los recursos necesarios para identificar fechas en español con los siguientes patrones (reglas):\n",
        "\n",
        "* Number + “de” +Month + “de” + Number\n",
        "   * 12 de agosto de 2006\n",
        "* Number\n",
        "   * 2008\n",
        "* Month + \"de\" + Number\n",
        "  * diciembre de 2023\n",
        "\n",
        "Cambiar la primera regla para que pueda identificar lo siguiente\n",
        "* [Day] + Number + “de” + month + [\"de\" + Number]\n",
        "   * Lunes 15 de marzo, martes 12 de junio de 2023, 12 de junio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK1ARnfwXbnT"
      },
      "outputs": [],
      "source": [
        "# creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    # first the pattern\n",
        "    AnnAt(\"Lookup\", features=dict(majorType=\"location\"),name=\"location1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"location1\", type=\"LOC1\", features=dict(rule=\"location1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(N(AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),min=1, max=2), N(AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),min=1,max=2), name=\"person1\"),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person1\", type=\"PER1\", features=dict(rule=\"person1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r3\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"person_first\")),\n",
        "            min=1, max=2\n",
        "          ),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"person2\"\n",
        "      ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person2\", type=\"PER2\", features=dict(rule=\"person2\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r4\n",
        "r4 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        N(\n",
        "            AnnAt(\"Lookup\", features=dict(majorType=\"surname\")),\n",
        "            min=1, max=2\n",
        "            ),\n",
        "        name=\"person3\"\n",
        "        ),\n",
        "    # then the action for the pattern\n",
        "    AddAnn(name=\"person3\", type=\"PER3\", features=dict(rule=\"person3\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r5\n",
        "r5 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(kind=\"number\")),\n",
        "        AnnAt(\"Token\", features=dict(string=\"de\")),\n",
        "        AnnAt(\"Lookup\", features=dict(minorType = \"month\")),\n",
        "        AnnAt(\"Token\", features=dict(string=\"de\")),\n",
        "        AnnAt(\"Token\", features=dict(kind=\"number\", length = 4, string = lambda x: int(x)>1900)),\n",
        "        name = \"date1\",\n",
        "        ),    # then the action for the pattern\n",
        "    AddAnn(name=\"date1\", type=\"DATE1\", features=dict(rule=\"date1\"))\n",
        "    )\n",
        "\n",
        "#Crear nueva regla r5\n",
        "r6 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        N(\n",
        "            Seq(\n",
        "                AnnAt(\"Token\", features=dict(kind=\"number\")),\n",
        "                AnnAt(\"Token\", features=dict(string=\"de\")),\n",
        "            ),\n",
        "            min=0, max=1\n",
        "        ),\n",
        "        AnnAt(\"Lookup\", features=dict(minorType=\"month\")),\n",
        "        N(\n",
        "            Seq(\n",
        "                AnnAt(\"Token\", features=dict(string=\"de\")),\n",
        "                AnnAt(\"Token\", features=dict(kind=\"number\", length=4, string=lambda x: int(x) > 1900)),\n",
        "            ),\n",
        "            min=0, max=1\n",
        "        ),\n",
        "        name=\"date2\",\n",
        "    ),  # then the action for the pattern\n",
        "    AddAnn(name=\"date2\", type=\"DATE2\", features=dict(rule=\"date2\"))\n",
        ")\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1, r2, r3, r4, r5, r6]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg7FHFeEYjeK"
      },
      "source": [
        "## Ejercicio 2 - Para entregar\n",
        "\n",
        "Crear los recursos necesarios para identificar localizaciones en español con los siguientes patrones (reglas):\n",
        "\n",
        "* “en” + Location\n",
        "  * en Murcia, en Orizaba\n",
        "* “en” + Token(upperInitial)\n",
        "  * en Murcia, en Orizaba\n",
        "* “en” + “el”|”la”|”los”|”las” + Token\n",
        "  * en el colegio, en la clase, en los botes, en las camas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uN0_g03Eu4x"
      },
      "outputs": [],
      "source": [
        "# Creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(string=\"en\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"location\")),\n",
        "        name=\"ejercicio1\",\n",
        "    ),\n",
        "    AddAnn(name=\"ejercicio1\", type=\"LOC1\", features=dict(rule=\"ejercicio1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(string=\"en\")),\n",
        "        AnnAt(\"Token\", features=dict(orth=\"upperInitial\")),\n",
        "        name=\"ejercicio2\",\n",
        "    ),\n",
        "    AddAnn(name=\"ejercicio2\", type=\"LOC2\", features=dict(rule=\"ejercicio2\"))\n",
        "    )\n",
        "\n",
        "r3 = Rule(\n",
        "    # first the pattern\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(string=\"en\")),\n",
        "        Or(\n",
        "            AnnAt(\"Token\", features=dict(string=\"el\")),\n",
        "            AnnAt(\"Token\", features=dict(string=\"la\")),\n",
        "            AnnAt(\"Token\", features=dict(string=\"los\")),\n",
        "            AnnAt(\"Token\", features=dict(string=\"las\")),\n",
        "            ),\n",
        "        AnnAt(\"Token\", features=dict(kind=\"word\")), #Esto es para evitar que coja números y que unicamente coja palabras.\n",
        "        name=\"ejercicio3\",\n",
        "    ),  # then the action for the pattern\n",
        "    AddAnn(name=\"ejercicio3\", type=\"LOC3\", features=dict(rule=\"ejercicio3\"))\n",
        ")\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1,r2,r3]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-D1xs4CIoFQ"
      },
      "source": [
        "## Ejercicio 3 - Para entregar\n",
        "\n",
        "Crear los recursos necesarios para identificar cantidades de dinero en español con los siguientes patrones (reglas):\n",
        "\n",
        "* Token.pos=\"NUM\" + Moneda (euros, dólares)\n",
        "  * 100.000 euros, 200.000 dólares\n",
        "* “\\$” + Token.pos = \"NUM\"\n",
        "  * $ 100.000\n",
        "* Token.pos=\"NUM\" + \"€\"\n",
        "  * 200.000 €\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2xUWjfEItIK"
      },
      "outputs": [],
      "source": [
        "# Creamos un gazetteer con el fichero descargado list.def\n",
        "gazetteer = StringGazetteer(source=\"gazetteer/lists.def\", source_fmt=\"gate-def\", outset_name=\"\",  ann_type=\"Lookup\")\n",
        "\n",
        "# eliminamos todas las anotaciones de tipo Lookup que ya existen actualmente\n",
        "doc.annset(\"\").remove(doc.annset(\"\").with_type(\"Lookup\"))\n",
        "\n",
        "# llamamos al gazetteer\n",
        "doc = gazetteer(doc)\n",
        "\n",
        "# eliminamos todas las anotaciones del conjunto \"Out1\"\n",
        "doc.annset(\"Out1\").clear()\n",
        "\n",
        "r1 = Rule(\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(pos = \"NUM\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"money\", minorType=\"name\")),\n",
        "        name=\"ejercicio3-1\",\n",
        "    ),\n",
        "    AddAnn(name=\"ejercicio3-1\", type=\"MON1\", features=dict(rule=\"ejercicio3-1\"))\n",
        "    )\n",
        "\n",
        "r2 = Rule(\n",
        "    Seq(\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"money\", minorType=\"icon\")),\n",
        "        AnnAt(\"Token\", features=dict(pos = \"NUM\")),\n",
        "        name=\"ejercicio3-2\",\n",
        "    ),\n",
        "    AddAnn(name=\"ejercicio3-2\", type=\"MON2\", features=dict(rule=\"ejercicio3-2\"))\n",
        "    )\n",
        "r3 = Rule(\n",
        "    Seq(\n",
        "        AnnAt(\"Token\", features=dict(pos = \"NUM\")),\n",
        "        AnnAt(\"Lookup\", features=dict(majorType=\"money\", minorType=\"icon\")),\n",
        "        name=\"ejercicio3-3\",\n",
        "    ),\n",
        "    AddAnn(name=\"ejercicio3-3\", type=\"MON3\", features=dict(rule=\"ejercicio3-3\"))\n",
        "    )\n",
        "\n",
        "# Create the annotation set for the annotations we want to match (just the tokens)\n",
        "anns2match = doc.annset(name=\"\").with_type(\"Token\", \"Lookup\")\n",
        "\n",
        "# Get the annotation set where we want to put new annotations\n",
        "outset = doc.annset(\"Out1\")\n",
        "\n",
        "# Create the Pampac instance from the single rule and run it on the annotations, also specify output set\n",
        "# The run method returns the list of offsets and the action return values where the rule matches in the doc\n",
        "rules =[r1,r2,r3]\n",
        "Pampac(*rules).run(doc, anns2match, outset=outset)\n",
        "doc\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
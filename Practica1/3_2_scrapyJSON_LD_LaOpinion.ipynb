{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TGINE/blob/main/Practica1/3_2_scrapyJSON_LD_LaOpinion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Sesión 3 - Scrapy - Ejemplo Crawling JSON-LD (ElMundo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de El Mundo extrayendo JSON-LD\n",
        "En el ejemplo siguiente definimos un crawler en Scrapy para extraer noticias de los metadatos de las páginas web.\n",
        "El Mundo y otros periódicos publican metadatos en formato JSON-LD (https://json-ld.org/) que permite obtener información estructurada de las webs. En este caso, tendremos que obtener esos objetos JSON-LD y extraer su información en el formato de noticia (https://schema.org/NewsArticle) publicado en Schema.org\n",
        "\n",
        "** Existen librerías de Python para trabajar directamente con JSON-LD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U scrapy"
      ],
      "metadata": {
        "id": "B2NYGvfRDtkA",
        "outputId": "00aa903f-ed0e-4982-cc19-7fe8f0248ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.8.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.1.2)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.1)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WEGeg3PzCZo"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class LaOpinionSpider (scrapy.Spider):\n",
        "\n",
        "    #Es obligatorio poner un nombre\n",
        "    name = 'laOpinion'\n",
        "\n",
        "    #Ponemos que el dominio que está permitido es el de la página y no vamos a irnos fuera de la misma\n",
        "    allowed_domains = ['www.laopiniondemurcia.es']\n",
        "\n",
        "    start_urls = ['https://www.laopiniondemurcia.es/deportes/2023/12/01/elpozo-murcia-busca-cabeza-serie-copa-futbol-sala-95322591.html']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    def parse (self, response):\n",
        "      \"\"\"\n",
        "      @inherit\n",
        "\n",
        "      @param self\n",
        "      @param response\n",
        "      \"\"\"\n",
        "\n",
        "      url = str(response.request.url).strip()\n",
        "      #Buscamos todos los elementos en el archivo XML con la etiqueta <item>\n",
        "      for item in response.css ('.bbnx-template.article-template'):\n",
        "\n",
        "            #Obtenemos por cada elemento <item> el texto del subelemento <title>. Además co el BeautifulSoup\n",
        "            #procesamos el texto en html y nos quedamos con el texto\n",
        "            title = BeautifulSoup(str(item.css ('.h1::text').get()), 'html.parser').get_text().strip()\n",
        "            #Obtenemos por cada elemento <item> el texto del subelemento <h2>\n",
        "            subtitle = BeautifulSoup(str(item.css ('.headline-article__extended-subtitle').get()), 'html.parser').get_text().strip()\n",
        "\n",
        "            author = BeautifulSoup(str(item.css('.news-author').get()), 'html.parser').get_text().strip()\n",
        "\n",
        "            date = BeautifulSoup(str(item.css(\".article-author__date\").get()), \"html.parser\").get_text().strip()\n",
        "\n",
        "            content = BeautifulSoup(str(item.css ('.article-body ').get()), 'html.parser').get_text().strip()\n",
        "\n",
        "            #Imprimimos la información obtenida para comprobar lo que estamos extrayendo\n",
        "            print (\"-------------------------\")\n",
        "            print ('URL:' + url)\n",
        "            print (\"Date : \"+date)\n",
        "            print ('Author:' + author)\n",
        "            print ('Título:' + title)\n",
        "            print ('Subtitulo:' + subtitle)\n",
        "            print ('Contenido:' + content)\n",
        "            print (\"-------------------------\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                \"date\": date,\n",
        "                'author': author,\n",
        "                'title': title,\n",
        "                'subtitle':subtitle,\n",
        "                'content': content,\n",
        "            }\n",
        "\n",
        "            #Creamos para cada item un fichero json y para ello obtenemos un número aleatorio.\n",
        "            filename = str(random.random()).replace(\".\",\"\") + \".json\"\n",
        "\n",
        "            # Si tenemos descripción, url y título entonces lo guardamos a disco en la carpeta 'rss'\n",
        "            if content and title and url:\n",
        "                with open ('rss/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "ZLMtdnS26NMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H27QM4rJCapR",
        "outputId": "6db87d27-d30f-4e42-d495-2fef4c30ca06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.11.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.5, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 86cf13266807bc65\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'LOG_ENABLED': False,\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.laopiniondemurcia.es/deportes/2023/12/01/elpozo-murcia-busca-cabeza-serie-copa-futbol-sala-95322591.html> (referer: None)\n",
            "ERROR:scrapy.core.scraper:Spider error processing <GET https://www.laopiniondemurcia.es/deportes/2023/12/01/elpozo-murcia-busca-cabeza-serie-copa-futbol-sala-95322591.html> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/twisted/internet/defer.py\", line 892, in _runCallbacks\n",
            "    current.result = callback(  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spiders/__init__.py\", line 79, in _parse\n",
            "    return self.parse(response, **kwargs)\n",
            "  File \"<ipython-input-2-6b32473a383e>\", line 74, in parse\n",
            "    with open ('rss/' + filename, 'w') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'rss/020473494206700094.json'\n",
            "INFO:scrapy.core.engine:Closing spider (finished)\n",
            "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 379,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 130305,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.935859,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 12, 1, 10, 25, 31, 564091, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 221038,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 140795904,\n",
            " 'memusage/startup': 140795904,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/FileNotFoundError': 1,\n",
            " 'start_time': datetime.datetime(2023, 12, 1, 10, 25, 30, 628232, tzinfo=datetime.timezone.utc)}\n",
            "INFO:scrapy.core.engine:Spider closed (finished)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "URL:https://www.laopiniondemurcia.es/deportes/2023/12/01/elpozo-murcia-busca-cabeza-serie-copa-futbol-sala-95322591.html\n",
            "Date : 01·12·23\n",
            "Author:Fran Montiel\n",
            "Título:ElPozo busca ser cabeza de serie en la Copa\n",
            "Subtitulo:Los de Javi Rodríguez visitan hoy al Santa Coloma con el objetivo de mantenerse entre los cuatro primeros\n",
            "Contenido:ElPozo Murcia afronta esta tarde su penúltimo partido de la primera vuelta ante el antiguo club de su técnico, Industrias Santa Coloma, en una nueva oportunidad para acercarse al liderato y distanciarse de sus perseguidores, a partir de las 19:30 horas.  Con la Copa de España ya asegurada bajo el brazo, los de Javi Rodríguez buscan ahora conseguir el segundo puesto y quitarse rápidamente de la cabeza el resultado del pasado fin de semana, empatando a dos ante un Ribera Navarra que venía en una dinámica muy mala situado en la parte baja de la tabla. \n",
            "\n",
            "\n",
            "\n",
            "        Una vez más, el problema fue la falta de ideas en tres cuarto de campo y el acierto de cara a portería, donde los murcianos estuvieron bastante espesos sobre todo en el primer tiempo dejando escapar una gran cantidad de oportunidades que acabaron pasando factura, obligando a los de la Región a conseguir un punto que supo a poco en los últimos minutos con el portero jugador.  Ahora, el momento de olvidar este resultado llega con una gran oportunidad para, en caso de sumar de tres, poner tierra de por medio con varios de sus rivales directos. Si los murcianos venciesen, adelantarían a Jimbee Cartagena haciéndose con el tercer puesto, quedando además a tres puntos del líder, el FC Barcelona.  \n",
            "\n",
            "\n",
            "        Además de mirar hacia arriba, también sería un paso importante con respecto a los que vienen apretando desde atrás, puesto que supondría meter cuatro puntos a Palma y distanciarse en diez de los puestos de fuera de playoff. Unos puestos que precisamente su rival tiene al alcance de la mano, estando décimos a un solo punto de Jaén, octavo.\n",
            "\n",
            "\n",
            "\n",
            "        El técnico Javi Rodríguez, en un duelo que será especial por su trayectoria, habló de la importancia en el aspecto personal que supone el duelo. «Soy colomense de nacimiento, siempre que puedo voy a Santa Coloma y cuando eres profesional vas a ganar donde sea, sin importar los sentimientos, estás en otro sitio y tienes que darlo todo por ese sitio», explicó.\n",
            "\n",
            "\n",
            "        En lo que respecta a la enfermería, destacó que poco a poco va vaciándose. «Estamos terminando con Felipe que pronto estará disponible, Rafa ya está bien y Marlon sigue estando fuera, el resto está bien y dando pasos adelante, si no están unos tendrán que demostrar los que juegan menos, esto es un mal para unos y un bien para otros», dijo Javi Rodríguez.\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('laOpinion')):\n",
        "    os.mkdir('laOpinion')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(LaOpinionSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
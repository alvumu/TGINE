{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TGINE/blob/main/Practica1/3_2_scrapyJSON_LD_digitalTrends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Scrapy rawling JSON-LD (DigitalTrends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de DigitalTrends extrayendo JSON-LD\n",
        "En el ejemplo siguiente definimos un crawler en Scrapy para extraer noticias de los metadatos de las páginas web.\n",
        "DigitalTrends y otros blogs publican metadatos en formato JSON-LD (https://json-ld.org/) que permite obtener información estructurada de las webs. En este caso, tendremos que obtener esos objetos JSON-LD y extraer su información en el formato de noticia (https://schema.org/NewsArticle) publicado en Schema.org\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U scrapy"
      ],
      "metadata": {
        "id": "B2NYGvfRDtkA",
        "outputId": "0854bc33-7fc9-4e0a-eead-1e0a39c7b0d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted<23.8.0,>=18.9.0 (from scrapy)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.7)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.11.17)\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-5.1.1 w3lib-2.1.2 zope.interface-6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WEGeg3PzCZo"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "\n",
        "import hashlib\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Para cada crawler que nos definimos nos debemos crear una clase Spider que debe heredar de la clase scrapy.Spider\n",
        "\n",
        "class DigitalTrendsSpider (scrapy.Spider):\n",
        "    name = 'DigitalTrends'\n",
        "\n",
        "    # Decimos que el dominio válido es el de la UM\n",
        "    allowed_domains = ['www.digitaltrends.com']\n",
        "\n",
        "    # podemos definir las páginas de inicio\n",
        "    start_urls = ['https://www.digitaltrends.com']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    nDocumentos = 0\n",
        "    visited_urls = set()\n",
        "\n",
        "    # debemos de implementar este método que se llamará para cada una de las páginas que se vayan analizando\n",
        "    def parse (self, response):\n",
        "        \"\"\"\n",
        "        @inherit\n",
        "\n",
        "        @param self\n",
        "        @param response\n",
        "        \"\"\"\n",
        "\n",
        "        if self.nDocumentos >= 1500:\n",
        "          return\n",
        "\n",
        "        # Guardamos la URL del sitio que se está visitando\n",
        "        url = str(response.request.url).strip()\n",
        "\n",
        "        # Cogemos el contenido relevante y para eso debemos usar selectores CSS\n",
        "        for article in response.css ('.b-page '):\n",
        "\n",
        "            # Cogemos el contenido del título\n",
        "            title = str (article.css ('.b-headline__title  ').get ()).strip()\n",
        "            title = BeautifulSoup (title, 'html.parser').get_text().strip()\n",
        "\n",
        "            # autor\n",
        "            autor = str (article.css ('.author.url.fn ').get ()).strip()\n",
        "            autor = BeautifulSoup (autor, 'html.parser').get_text().strip()\n",
        "\n",
        "            # Date\n",
        "            date = str (article.css ('.b-byline__time.date.dtreviewed').get ()).strip()\n",
        "            date = BeautifulSoup (date, 'html.parser').get_text().strip()\n",
        "\n",
        "            #Content\n",
        "            content = \"\".join (article.css ('.b-content.b-single__content.h-article-content ').get ())\n",
        "            content = BeautifulSoup (content, 'html.parser').get_text().strip().replace(\"\\\"\",\"\").replace(\"\\n\",\"\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'autor': autor,\n",
        "                'date': date,\n",
        "                'content': content\n",
        "            }\n",
        "\n",
        "\n",
        "            title_hash = hashlib.md5(title.encode()).hexdigest()\n",
        "\n",
        "            filename = f\"{title_hash}.json\"\n",
        "\n",
        "            # Guardamos el documento si tiene contenido y título\n",
        "            if content and title:\n",
        "                print (\"-------------------------\")\n",
        "                print (url)\n",
        "                print (title)\n",
        "                print (autor)\n",
        "                print (date)\n",
        "                print (content)\n",
        "                print (\"-------------------------\")\n",
        "                self.nDocumentos = self.nDocumentos + 1\n",
        "                with open ('digitalTrends/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)\n",
        "\n",
        "\n",
        "\n",
        "        # Obtenemos todas las otros links de la página representados por la etiqueta <a>\n",
        "\n",
        "\n",
        "        url_in_current_document = response.css ('a')\n",
        "\n",
        "        for next_page in url_in_current_document:\n",
        "            # Para limitar que solamente se parseen las noticias dentro de 'https://www.digitaltrends.com/computing/ o https://www.digitaltrends.com/mobile/'\n",
        "            # obtenemos el atributo href de la etiqueta <a> y parseamos la página\n",
        "\n",
        "            url_str = str(next_page.css('::attr(href)').get())\n",
        "\n",
        "\n",
        "            if (\"https://www.digitaltrends.com/computing/\" in url_str) or (\"/mobile/\" in url_str) and url_str not in self.visited_urls and self.nDocumentos < 1500:\n",
        "              self.visited_urls.add(url_str)\n",
        "              yield response.follow (next_page, self.parse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "ZLMtdnS26NMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H27QM4rJCapR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('digitalTrends')):\n",
        "    os.mkdir('digitalTrends')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(DigitalTrendsSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_archivos_en_carpeta(ruta):\n",
        "    try:\n",
        "        # Lista todos los archivos en la carpeta\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        # Filtra solo los archivos (no directorios)\n",
        "        archivos = [archivo for archivo in archivos if os.path.isfile(os.path.join(ruta, archivo))]\n",
        "\n",
        "        # Devuelve el número de archivos\n",
        "        return len(archivos)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al contar archivos: {e}\")\n",
        "        return None\n",
        "\n",
        "contar_archivos_en_carpeta(\"digitalTrends\")"
      ],
      "metadata": {
        "id": "Iqc7lll3MbL4",
        "outputId": "c70a0268-10df-497e-f650-80364e37be32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1500"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 2 – Buscador (0,75 puntos)\n"
      ],
      "metadata": {
        "id": "exxHlc9_TnWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whoosh"
      ],
      "metadata": {
        "id": "Fo8DaGgdTko5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72400f89-cafd-40bc-d1c0-2be5f2df6b7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/468.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/468.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT, ID, DATETIME\n",
        "from whoosh.qparser import QueryParser\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "import re\n",
        "\n",
        "# Define el esquema para el índice de Whoosh\n",
        "schema = Schema(\n",
        "    url=ID(unique=True),\n",
        "    title=TEXT(stored=True),\n",
        "    author=TEXT(stored=True),\n",
        "    date=DATETIME(stored=True),\n",
        "    content=TEXT(stored=True)\n",
        ")\n",
        "\n",
        "index_dir = \"whooshIndex2\"\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "    ix = create_in(index_dir, schema)\n",
        "else:\n",
        "    ix = open_dir(index_dir)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RQlA7bYNpM7F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrega los documentos al índice\n",
        "# Aquí deberías leer tus archivos y agregarlos al índice\n",
        "# Utiliza el bloque 'with' para garantizar que el índice se cierre correctamente\n",
        "with ix.writer() as writer:\n",
        "# Recorre todos los archivos en el directorio digitalTrends\n",
        "  for filename in os.listdir('digitalTrends'):\n",
        "      if filename.endswith(\".json\"):  # Asegúrate de que sean archivos JSON\n",
        "          file_path = os.path.join('digitalTrends', filename)\n",
        "          with open(file_path, 'r', encoding='utf-8') as file:\n",
        "              try:\n",
        "                  file_data = json.load(file)\n",
        "                  date_string_clean = re.sub(r'\\s+\\d+:\\d+[A|P]M', '', file_data['date'])\n",
        "                  if date_string_clean != \"None\" :\n",
        "                    date_obj = datetime.strptime(date_string_clean, '%B %d, %Y')\n",
        "                  else :\n",
        "                    date_obj = None\n",
        "                  # Agrega los documentos al índice\n",
        "                  writer.add_document(\n",
        "                     url=file_data['url'],\n",
        "                     title=file_data['title'],\n",
        "                     author=file_data['autor'],\n",
        "                     date = date_obj,\n",
        "                     content=file_data['content']\n",
        "                  )\n",
        "              except json.JSONDecodeError as e:\n",
        "                  print(f\"Error al procesar el archivo {filename}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A3BW_FYB1XPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import open_dir\n",
        "from whoosh.qparser import QueryParser\n",
        "from whoosh import scoring\n",
        "\n",
        "# Abre el índice creado\n",
        "ix = open_dir(index_dir)\n",
        "\n",
        "# Define el parser de consultas\n",
        "parser = QueryParser(\"content\", schema=ix.schema)\n",
        "\n",
        "# Ejemplo de búsqueda básica con operadores lógicos\n",
        "with ix.searcher(weighting=scoring.BM25F) as searcher:\n",
        "    # Ejemplo de búsqueda con operadores lógicos (AND, OR, NOT)\n",
        "    query = parser.parse(\"(Pixel AND Watch) OR Google NOT Apple\")\n",
        "\n",
        "    # Ejecuta la búsqueda\n",
        "    results = searcher.search(query)\n",
        "\n",
        "    # Imprime los resultados\n",
        "    for hit in results:\n",
        "        print(hit['title'])\n",
        "        print(hit.highlights(\"content\"))  # Resalta las frases coincidentes\n"
      ],
      "metadata": {
        "id": "ueeVIuVbu5Xk",
        "outputId": "bd85e754-2221-4932-8eed-87c6c181359f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forget waiting for Google I/O — the Pixel Fold just had a huge leak\n",
            "channel titled “<b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> x NBA: The Greatest...Furthermore, the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Twitter account shared...bad\t\t\t\t\t\t\t\t\tDoes the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> work with an iPhone\n",
            "Forget waiting for Google I/O — the Pixel Fold just had a huge leak\n",
            "anticipated is the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold. And ahead of the...bad\t\t\t\t\t\t\t\t\tDoes the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> work with an iPhone...I used to love the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold. Now, I’m not so sure\n",
            "There’s a new Pixel Tablet coming in 2023\n",
            "with the upcoming <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>, this new <b class=\"match term1\">Pixel</b> Tablet effectively...I used to love the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold. Now, I’m not so sure...year\t\t\t\t\t\t\t\t\tThe <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> 2 just got a feature\n",
            "The Google Pixel Fold looks incredible in its first major design leak\n",
            "like that of the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> 7 Pro. Camera specs...during next year’s <b class=\"match term0\">Google</b> I/O event).<b class=\"match term3\">GOOGLE</b> <b class=\"match term4\">PIXEL</b> FOLD - EXCLUSIVE FIRST...Does the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> work with an iPhone\n",
            "The Google Pixel Fold looks incredible in its first major design leak\n",
            "during next year’s <b class=\"match term0\">Google</b> I/O event).<b class=\"match term3\">GOOGLE</b> <b class=\"match term4\">PIXEL</b> FOLD - EXCLUSIVE FIRST...reports that the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold will retail for...Does the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> work with an iPhone\n",
            "Google Fi finally gets an unlimited data plan … but read the fine print\n",
            "<b class=\"match term0\">Google</b> is finally making its <b class=\"match term0\">Google</b> Fi, its mobile virtual...issue with its <b class=\"match term1\">Pixel</b> phones\t\t\t\t\t\t\t\t\tThe <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> is finally getting...for\t\t\t\t\t\t\t\t\tIs the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> waterproof? Read this\n",
            "Google Fi finally gets an unlimited data plan … but read the fine print\n",
            "you can get a <b class=\"match term1\">Pixel</b> 3 or <b class=\"match term1\">Pixel</b> 3 XL at half price if...issue with its <b class=\"match term1\">Pixel</b> phones\t\t\t\t\t\t\t\t\tThe <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> is finally getting...for\t\t\t\t\t\t\t\t\tIs the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> waterproof? Read this\n",
            "Google Pixel 7 vs. Google Pixel 6: What’s changed?\n",
            "this time, like the <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>, it’s the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> 7 people are scrambling...knocks $80 off the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>\t\t\t\t\t\t\t\t\t<b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold just got a huge...Best <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> deals: Save on <b class=\"match term1\">Pixel</b> 8, <b class=\"match term1\">Pixel</b> Buds, and <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>\t\t\t\t\t\t\t\t\tBlack Friday is over\n",
            "Google Pixel 7 vs. Google Pixel 6: What’s changed?\n",
            "this time, like the <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>, it’s the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> 7 people are scrambling...knocks $80 off the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>\t\t\t\t\t\t\t\t\t<b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> Fold just got a huge...Best <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> deals: Save on <b class=\"match term1\">Pixel</b> 8, <b class=\"match term1\">Pixel</b> Buds, and <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>\t\t\t\t\t\t\t\t\tBlack Friday is over\n",
            "Google finally adds streaming to YouTube Music on Wear OS\n",
            "launch of the Galaxy <b class=\"match term2\">Watch</b> 4 and Galaxy <b class=\"match term2\">Watch</b> 4 Classic last year...recently announced a <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b>, as well as a partnership...Does the <b class=\"match term0\">Google</b> <b class=\"match term1\">Pixel</b> <b class=\"match term2\">Watch</b> work with an iPhone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de búsqueda por rango de fechas y orden de resultados\n",
        "from whoosh.qparser.dateparse import DateParserPlugin\n",
        "\n",
        "# Añade el plugin para búsqueda por rango de fechas\n",
        "parser.add_plugin(DateParserPlugin())\n",
        "\n",
        "with ix.searcher() as searcher:\n",
        "    # Ejemplo de búsqueda por rango de fechas\n",
        "    query = parser.parse(\"date:[2022-01-01 TO 2022-12-31]\")\n",
        "\n",
        "    # Ejecuta la búsqueda con orden de resultados por fecha (ascendente)\n",
        "    results = searcher.search(query, sortedby=\"date\")\n",
        "\n",
        "    # Imprime los resultados ordenados por fecha\n",
        "    for hit in results:\n",
        "        print(hit['title'], hit['date'])"
      ],
      "metadata": {
        "id": "dWJ1c0v8z_Kk",
        "outputId": "b80f5400-a4a5-4bf4-837f-2ade25365ce3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scosche unveils several new iPhone MagSafe car mounts at CES 2022 2022-01-03 00:00:00\n",
            "AT&T and Verizon forge ahead with new 5G deployments despite aviation industry fears 2022-01-03 00:00:00\n",
            "Scosche unveils several new iPhone MagSafe car mounts at CES 2022 2022-01-03 00:00:00\n",
            "AT&T and Verizon forge ahead with new 5G deployments despite aviation industry fears 2022-01-03 00:00:00\n",
            "This wild new display puts a gargantuan 120-inch virtual monitor on your desk 2022-01-04 00:00:00\n",
            "Microsoft Surface Duo reportedly getting Android 12L despite missing Android 11 2022-01-04 00:00:00\n",
            "I don’t care if the Asus ROG Flow Z13 is a laptop or a tablet — I just want one 2022-01-04 00:00:00\n",
            "AT&T’s Fusion 5G is the carrier’s newest affordable 5G smartphone 2022-01-04 00:00:00\n",
            "How to customize your iPhone backup in iCloud 2022-01-04 00:00:00\n",
            "TP-Link’s Wi-Fi 6E router comes with motorized antennas for better reception 2022-01-04 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.qparser import QueryParser\n",
        "from whoosh.highlight import Highlighter, ContextFragmenter\n",
        "# Realiza una búsqueda\n",
        "with ix.searcher() as searcher:\n",
        "    query = parser.parse(\"Google\")  # Reemplaza 'your_query_here' con tu consulta\n",
        "    results = searcher.search(query)\n",
        "\n",
        "    # Configura el fragmenter para resaltar fragmentos de texto\n",
        "    fragmenter = ContextFragmenter(maxchars=200, surround=30)\n",
        "    highlighter = Highlighter(fragmenter=fragmenter)\n",
        "\n",
        "    # Imprime los resultados resaltados\n",
        "    for hit in results:\n",
        "        # Obtiene el texto resaltado del campo 'content'\n",
        "        text = hit.highlights(\"content\", top=2)  # top: Número máximo de fragmentos resaltados por documento\n",
        "\n",
        "        # Imprime los resultados resaltados\n",
        "        print(f\"Documento: {hit['title']}\")\n",
        "        print(f\"Fragmentos resaltados: {text}\")\n"
      ],
      "metadata": {
        "id": "ZRuuKnVD594e",
        "outputId": "ad7e4068-ecf6-4b4d-ad45-841e9740c593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento: Which Google One subscription is right for you? Google’s paid plans explained\n",
            "Fragmentos resaltados: across <b class=\"match term0\">Google</b> Drive, <b class=\"match term0\">Google</b> Photos, and Gmail...Videos\t\tIsn’t <b class=\"match term0\">Google</b> One the same as <b class=\"match term0\">Google</b> Drive?On June 1, <b class=\"match term0\">Google</b> capped unlimited\n",
            "Documento: Which Google One subscription is right for you? Google’s paid plans explained\n",
            "Fragmentos resaltados: <b class=\"match term0\">Google</b> One plans have?Sharing your <b class=\"match term0\">Google</b> One plan with family...Videos\t\tIsn’t <b class=\"match term0\">Google</b> One the same as <b class=\"match term0\">Google</b> Drive?On June 1, <b class=\"match term0\">Google</b> capped unlimited\n",
            "Documento: Heads up — your Google account may get deleted next month\n",
            "Fragmentos resaltados: surprising. In May, <b class=\"match term0\">Google</b> announced that accounts...the deletion of a <b class=\"match term0\">Google</b> account, all critical\n",
            "Documento: Heads up — your Google account may get deleted next month\n",
            "Fragmentos resaltados: TrendsOwners of a <b class=\"match term0\">Google</b> account that has...the deletion of a <b class=\"match term0\">Google</b> account, all critical\n",
            "Documento: Google is killing another one of its popular apps, and it’s a big one\n",
            "Fragmentos resaltados: including <b class=\"match term0\">Google</b> Domains, <b class=\"match term0\">Google</b> Optimize, <b class=\"match term0\">Google</b> Album Archive, YouTube...YouTube Originals, <b class=\"match term0\">Google</b> Currents, <b class=\"match term0\">Google</b> Street View, and many\n",
            "Documento: Google is killing another one of its popular apps, and it’s a big one\n",
            "Fragmentos resaltados: including <b class=\"match term0\">Google</b> Domains, <b class=\"match term0\">Google</b> Optimize, <b class=\"match term0\">Google</b> Album Archive, YouTube...YouTube Originals, <b class=\"match term0\">Google</b> Currents, <b class=\"match term0\">Google</b> Street View, and many\n",
            "Documento: The Google Pixel Tablet is a bad idea that might just work\n",
            "Fragmentos resaltados: <b class=\"match term0\">Google</b> is bringing a strange new addition to its...<b class=\"match term0\">Google</b>’s hardware event for the Pixel 7, we got an even\n",
            "Documento: The Google Pixel Tablet is a bad idea that might just work\n",
            "Fragmentos resaltados: <b class=\"match term0\">Google</b> is bringing a strange new addition to its...initially teased at <b class=\"match term0\">Google</b> I/O in May. And during\n",
            "Documento: 5 things we’d love to see at Google I/O 2023 (but probably won’t)\n",
            "Fragmentos resaltados: part of our complete <b class=\"match term0\">Google</b> I/O coverage <b class=\"match term0\">Google</b>’s annual developer...you off, though, as <b class=\"match term0\">Google</b> I/O is one of the biggest\n",
            "Documento: 5 things we’d love to see at Google I/O 2023 (but probably won’t)\n",
            "Fragmentos resaltados: part of our complete <b class=\"match term0\">Google</b> I/O coverage <b class=\"match term0\">Google</b>’s annual developer...conference, <b class=\"match term0\">Google</b> I/O, kicks off on May\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "im_CZOsq8oOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
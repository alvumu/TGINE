{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TGINE/blob/main/Practica1/3_2_scrapyJSON_LD_digitalTrends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Scrapy rawling JSON-LD (DigitalTrends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de DigitalTrends extrayendo JSON-LD\n",
        "En el ejemplo siguiente definimos un crawler en Scrapy para extraer noticias de los metadatos de las páginas web.\n",
        "DigitalTrends y otros blogs publican metadatos en formato JSON-LD (https://json-ld.org/) que permite obtener información estructurada de las webs. En este caso, tendremos que obtener esos objetos JSON-LD y extraer su información en el formato de noticia (https://schema.org/NewsArticle) publicado en Schema.org\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U scrapy"
      ],
      "metadata": {
        "id": "B2NYGvfRDtkA",
        "outputId": "66e7c47c-9415-457c-dc54-5420f680f4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted<23.8.0,>=18.9.0 (from scrapy)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.7)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.11.17)\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-5.1.1 w3lib-2.1.2 zope.interface-6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WEGeg3PzCZo"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "\n",
        "import hashlib\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Para cada crawler que nos definimos nos debemos crear una clase Spider que debe heredar de la clase scrapy.Spider\n",
        "\n",
        "class DigitalTrendsSpider (scrapy.Spider):\n",
        "    name = 'DigitalTrends'\n",
        "\n",
        "    # Decimos que el dominio válido es el de la UM\n",
        "    allowed_domains = ['www.digitaltrends.com']\n",
        "\n",
        "    # podemos definir las páginas de inicio\n",
        "    start_urls = ['https://www.digitaltrends.com']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    nDocumentos = 0\n",
        "    visited_urls = set()\n",
        "\n",
        "    # debemos de implementar este método que se llamará para cada una de las páginas que se vayan analizando\n",
        "    def parse (self, response):\n",
        "        \"\"\"\n",
        "        @inherit\n",
        "\n",
        "        @param self\n",
        "        @param response\n",
        "        \"\"\"\n",
        "\n",
        "        if self.nDocumentos >= 1500:\n",
        "          return\n",
        "\n",
        "        # Guardamos la URL del sitio que se está visitando\n",
        "        url = str(response.request.url).strip()\n",
        "\n",
        "        # Cogemos el contenido relevante y para eso debemos usar selectores CSS\n",
        "        for article in response.css ('.b-page '):\n",
        "\n",
        "            # Cogemos el contenido del título\n",
        "            title = str (article.css ('.b-headline__title  ').get ()).strip()\n",
        "            title = BeautifulSoup (title, 'html.parser').get_text().strip()\n",
        "\n",
        "            # autor\n",
        "            autor = str (article.css ('.author.url.fn ').get ()).strip()\n",
        "            autor = BeautifulSoup (autor, 'html.parser').get_text().strip()\n",
        "\n",
        "            # Date\n",
        "            date = str (article.css ('.b-byline__time.date.dtreviewed').get ()).strip()\n",
        "            date = BeautifulSoup (date, 'html.parser').get_text().strip()\n",
        "\n",
        "            #Content\n",
        "            content = \"\".join (article.css ('.b-content.b-single__content.h-article-content ').get ())\n",
        "            content = BeautifulSoup (content, 'html.parser').get_text().strip().replace(\"\\\"\",\"\").replace(\"\\n\",\"\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'autor': autor,\n",
        "                'date': date,\n",
        "                'content': content\n",
        "            }\n",
        "\n",
        "\n",
        "            title_hash = hashlib.md5(title.encode()).hexdigest()\n",
        "\n",
        "            filename = f\"{title_hash}.json\"\n",
        "\n",
        "            # Guardamos el documento si tiene contenido y título\n",
        "            if content and title:\n",
        "                print (\"-------------------------\")\n",
        "                print (url)\n",
        "                print (title)\n",
        "                print (autor)\n",
        "                print (date)\n",
        "                print (content)\n",
        "                print (\"-------------------------\")\n",
        "                self.nDocumentos = self.nDocumentos + 1\n",
        "                with open ('digitalTrends/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)\n",
        "\n",
        "\n",
        "\n",
        "        # Obtenemos todas las otros links de la página representados por la etiqueta <a>\n",
        "\n",
        "\n",
        "        url_in_current_document = response.css ('a')\n",
        "\n",
        "        for next_page in url_in_current_document:\n",
        "            # Para limitar que solamente se parseen las noticias dentro de 'https://www.digitaltrends.com/computing/ o https://www.digitaltrends.com/mobile/'\n",
        "            # obtenemos el atributo href de la etiqueta <a> y parseamos la página\n",
        "\n",
        "            url_str = str(next_page.css('::attr(href)').get())\n",
        "\n",
        "\n",
        "            if (\"https://www.digitaltrends.com/computing/\" in url_str) or (\"/mobile/\" in url_str) and url_str not in self.visited_urls and self.nDocumentos < 1500:\n",
        "              self.visited_urls.add(url_str)\n",
        "              yield response.follow (next_page, self.parse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "ZLMtdnS26NMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H27QM4rJCapR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('digitalTrends')):\n",
        "    os.mkdir('digitalTrends')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(DigitalTrendsSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_archivos_en_carpeta(ruta):\n",
        "    try:\n",
        "        # Lista todos los archivos en la carpeta\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        # Filtra solo los archivos (no directorios)\n",
        "        archivos = [archivo for archivo in archivos if os.path.isfile(os.path.join(ruta, archivo))]\n",
        "\n",
        "        # Devuelve el número de archivos\n",
        "        return len(archivos)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al contar archivos: {e}\")\n",
        "        return None\n",
        "\n",
        "contar_archivos_en_carpeta(\"digitalTrends\")"
      ],
      "metadata": {
        "id": "Iqc7lll3MbL4",
        "outputId": "ab680e9c-47fe-4bd1-a3f6-7dcaf9ce2f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1500"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 2 – Buscador (0,75 puntos)\n"
      ],
      "metadata": {
        "id": "exxHlc9_TnWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install whoosh"
      ],
      "metadata": {
        "id": "Fo8DaGgdTko5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e7eeba6-b07f-472b-ec6a-77d27ddaec95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/468.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/468.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m460.8/468.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: whoosh\n",
            "Successfully installed whoosh-2.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import create_in, open_dir\n",
        "from whoosh.fields import Schema, TEXT, ID, DATETIME\n",
        "from whoosh.qparser import QueryParser\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n"
      ],
      "metadata": {
        "id": "RQlA7bYNpM7F"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download(\"punkt\")\n",
        "ps = PorterStemmer()\n",
        "# Example inflections to reduce\n",
        "example_words = [\"gaming\"]\n",
        "# Perform stemming\n",
        "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in example_words:\n",
        "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Fp4Idgt8Y5",
        "outputId": "a968728b-5640-4610-88d9-76514bc1fe54"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Word--            --Stem--            \n",
            "gaming              game                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el esquema para el índice de Whoosh\n",
        "schema = Schema(\n",
        "    url=ID(unique=True, analyzer = PorterStemmer()),\n",
        "    title=TEXT(stored=True,analyzer = PorterStemmer()),\n",
        "    author=TEXT(stored=True,analyzer = PorterStemmer()),\n",
        "    date=DATETIME(stored=True),\n",
        "    content=TEXT(stored=True,analyzer = PorterStemmer())\n",
        ")\n",
        "\n",
        "index_dir = \"whooshIndex\"\n",
        "if not os.path.exists(index_dir):\n",
        "    os.mkdir(index_dir)\n",
        "    ix = create_in(index_dir, schema)\n",
        "else:\n",
        "    ix = open_dir(index_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "12ZAOkPMrEWa"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrega los documentos al índice\n",
        "# Aquí deberías leer tus archivos y agregarlos al índice\n",
        "# Utiliza el bloque 'with' para garantizar que el índice se cierre correctamente\n",
        "with ix.writer() as writer:\n",
        "# Recorre todos los archivos en el directorio digitalTrends\n",
        "  for filename in os.listdir('digitalTrends'):\n",
        "      if filename.endswith(\".json\"):  # Asegúrate de que sean archivos JSON\n",
        "          file_path = os.path.join('digitalTrends', filename)\n",
        "          with open(file_path, 'r', encoding='utf-8') as file:\n",
        "              try:\n",
        "                  file_data = json.load(file)\n",
        "                  date_string_clean = re.sub(r'\\s+\\d+:\\d+[A|P]M', '', file_data['date'])\n",
        "                  if date_string_clean != \"None\" :\n",
        "                    date_obj = datetime.strptime(date_string_clean, '%B %d, %Y')\n",
        "                  else :\n",
        "                    date_obj = None\n",
        "                  # Agrega los documentos al índice\n",
        "                  writer.add_document(\n",
        "                     url=file_data['url'],\n",
        "                     title=file_data['title'],\n",
        "                     author=file_data['autor'],\n",
        "                     date = date_obj,\n",
        "                     content=file_data['content']\n",
        "                  )\n",
        "              except json.JSONDecodeError as e:\n",
        "                  print(f\"Error al procesar el archivo {filename}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "A3BW_FYB1XPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "36f89d41-a822-4edf-fb6b-be5cd4904856"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-28f8654fc63e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mdate_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                   \u001b[0;31m# Agrega los documentos al índice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                   writer.add_document(\n\u001b[0m\u001b[1;32m     19\u001b[0m                      \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                      \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whoosh/writing.py\u001b[0m in \u001b[0;36madd_document\u001b[0;34m(self, **fields)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mscorable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;31m# Add the terms to the pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mtbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvbytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                     \u001b[0mweight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfieldboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mscorable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whoosh/fields.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, value, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mword_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mana\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvbytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mana\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mutf8encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whoosh/formats.py\u001b[0m in \u001b[0;36mword_values\u001b[0;34m(self, value, analyzer, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"positions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boosts\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0mposes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whoosh/formats.py\u001b[0m in \u001b[0;36mtokens\u001b[0;34m(value, analyzer, kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentoken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munstopped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'PorterStemmer' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whoosh.index import open_dir\n",
        "from whoosh.qparser import QueryParser,FuzzyTermPlugin\n",
        "from whoosh import scoring\n",
        "from whoosh.spelling import QueryCorrector\n",
        "from whoosh.highlight import Highlighter, ContextFragmenter\n",
        "from whoosh.qparser.dateparse import DateParserPlugin\n",
        "from whoosh.analysis import StemmingAnalyzer\n",
        "\n",
        "\n",
        "\n",
        "# Función para realizar búsquedas\n",
        "def buscar_en_archivos(queryWords=\"\", queryDate=\"\", sortedBy=\"date\", directory='whooshIndex2'):\n",
        "    # Se abre el índice existente o se crea uno nuevo si no existe\n",
        "    ix = open_dir(directory)\n",
        "\n",
        "    # Se crea un QueryParser para buscar en los campos específicos\n",
        "    with ix.searcher(weighting=scoring.BM25F()) as searcher:\n",
        "        #Indicamos que el parser va a ser sobre el campo content\n",
        "        parser = QueryParser(\"content\", schema=ix.schema)\n",
        "        #Añadimos los plugins necesarios para las funcionalidades descritas\n",
        "        #Plugin valido para realizar las busquedas por fechas\n",
        "        parser.add_plugin(DateParserPlugin())\n",
        "        # Añadir Stemming al parser para búsqueda más flexible\n",
        "        parser.add_plugin(FuzzyTermPlugin())\n",
        "        # Se inicializa la consulta combinada con la búsqueda por palabras y fecha\n",
        "        user_query = parser.parse(queryWords)  # Búsqueda por palabras\n",
        "        if queryDate:  # Si se proporciona una fecha, se agrega a la consulta\n",
        "            date_query = parser.parse(queryDate)\n",
        "            user_query = user_query & date_query\n",
        "\n",
        "       # Se corrige la consulta en caso de que esté mal realizada\n",
        "        correction = searcher.correct_query(q=user_query,qstring=queryWords)\n",
        "        if correction.query != user_query:\n",
        "          print(\"Did you mean:\", correction.string)\n",
        "        # Se procede a realizar la busqueda\n",
        "        search_results = searcher.search(correction.query, limit=None, sortedby=sortedBy)\n",
        "        # Configuramos el fragmenter para resaltar fragmentos de texto\n",
        "        fragmenter = ContextFragmenter(maxchars=200, surround=30)\n",
        "        highlighter = Highlighter(fragmenter=fragmenter)\n",
        "        # Mostramos los resultados de las noticias que se han encontrado según las condiciones del usuario\n",
        "        for hit in search_results:\n",
        "            print(f\"Título: {hit['title']}\")\n",
        "            print(f\"Autor: {hit['author']}\")\n",
        "            print(f\"Fecha: {hit['date']}\")\n",
        "            print(f\"Fragmento de contenido: {hit.highlights('content')}\")\n",
        "            print(\"--------------\")\n"
      ],
      "metadata": {
        "id": "ueeVIuVbu5Xk"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llamar a la función para realizar las búsquedas\n",
        "buscar_en_archivos(queryWords=\"game\", queryDate=\"date:[2022-01-01 TO 2022-12-28]\", sortedBy=\"author\")\n"
      ],
      "metadata": {
        "id": "mIvr7n-DZDvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
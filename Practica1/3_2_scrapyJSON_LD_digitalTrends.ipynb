{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/TGINE/blob/main/Practica1/3_2_scrapyJSON_LD_digitalTrends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Scrapy rawling JSON-LD (DigitalTrends)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de DigitalTrends extrayendo JSON-LD\n",
        "En el ejemplo siguiente definimos un crawler en Scrapy para extraer noticias de los metadatos de las páginas web.\n",
        "DigitalTrends y otros blogs publican metadatos en formato JSON-LD (https://json-ld.org/) que permite obtener información estructurada de las webs. En este caso, tendremos que obtener esos objetos JSON-LD y extraer su información en el formato de noticia (https://schema.org/NewsArticle) publicado en Schema.org\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U scrapy"
      ],
      "metadata": {
        "id": "B2NYGvfRDtkA",
        "outputId": "5204c4cd-1f22-4cdc-f09a-27d14bafe378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.7)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.8.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.1.2)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.1)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WEGeg3PzCZo"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "\n",
        "import hashlib\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Para cada crawler que nos definimos nos debemos crear una clase Spider que debe heredar de la clase scrapy.Spider\n",
        "\n",
        "class DigitalTrendsSpider (scrapy.Spider):\n",
        "    name = 'DigitalTrends'\n",
        "\n",
        "    # Decimos que el dominio válido es el de la UM\n",
        "    allowed_domains = ['www.digitaltrends.com']\n",
        "\n",
        "    # podemos definir las páginas de inicio\n",
        "    start_urls = ['https://www.digitaltrends.com']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    nDocumentos = 0\n",
        "    visited_urls = set()\n",
        "\n",
        "    # debemos de implementar este método que se llamará para cada una de las páginas que se vayan analizando\n",
        "    def parse (self, response):\n",
        "        \"\"\"\n",
        "        @inherit\n",
        "\n",
        "        @param self\n",
        "        @param response\n",
        "        \"\"\"\n",
        "\n",
        "        if self.nDocumentos >= 1500:\n",
        "          return\n",
        "\n",
        "        # Guardamos la URL del sitio que se está visitando\n",
        "        url = str(response.request.url).strip()\n",
        "\n",
        "        # Cogemos el contenido relevante y para eso debemos usar selectores CSS\n",
        "        for article in response.css ('.b-page '):\n",
        "\n",
        "            # Cogemos el contenido del título\n",
        "            title = str (article.css ('.b-headline__title  ').get ()).strip()\n",
        "            title = BeautifulSoup (title, 'html.parser').get_text().strip()\n",
        "\n",
        "            # autor\n",
        "            autor = str (article.css ('.author.url.fn ').get ()).strip()\n",
        "            autor = BeautifulSoup (autor, 'html.parser').get_text().strip()\n",
        "\n",
        "            # Date\n",
        "            date = str (article.css ('.b-byline__time.date.dtreviewed').get ()).strip()\n",
        "            date = BeautifulSoup (date, 'html.parser').get_text().strip()\n",
        "\n",
        "            #Content\n",
        "            content = \"\".join (article.css ('.b-content.b-single__content.h-article-content ').get ())\n",
        "            content = BeautifulSoup (content, 'html.parser').get_text().strip().replace(\"\\\"\",\"\").replace(\"\\n\",\"\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'autor': autor,\n",
        "                'date': date,\n",
        "                'content': content\n",
        "            }\n",
        "\n",
        "\n",
        "            title_hash = hashlib.md5(title.encode()).hexdigest()\n",
        "\n",
        "            filename = f\"{title_hash}.json\"\n",
        "\n",
        "            # Guardamos el documento si tiene contenido y título\n",
        "            if content and title:\n",
        "                print (\"-------------------------\")\n",
        "                print (url)\n",
        "                print (title)\n",
        "                print (autor)\n",
        "                print (date)\n",
        "                print (content)\n",
        "                print (\"-------------------------\")\n",
        "                self.nDocumentos = self.nDocumentos + 1\n",
        "                with open ('digitalTrends/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)\n",
        "\n",
        "\n",
        "\n",
        "        # Obtenemos todas las otros links de la página representados por la etiqueta <a>\n",
        "\n",
        "\n",
        "        url_in_current_document = response.css ('a')\n",
        "\n",
        "        for next_page in url_in_current_document:\n",
        "            # Para limitar que solamente se parseen las noticias dentro de 'https://www.digitaltrends.com/computing/ o https://www.digitaltrends.com/mobile/'\n",
        "            # obtenemos el atributo href de la etiqueta <a> y parseamos la página\n",
        "\n",
        "            url_str = str(next_page.css('::attr(href)').get())\n",
        "\n",
        "\n",
        "            if (\"https://www.digitaltrends.com/computing/\" in url_str) or (\"/mobile/\" in url_str) and url_str not in self.visited_urls and self.nDocumentos < 1500:\n",
        "              self.visited_urls.add(url_str)\n",
        "              yield response.follow (next_page, self.parse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "ZLMtdnS26NMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H27QM4rJCapR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('digitalTrends')):\n",
        "    os.mkdir('digitalTrends')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(DigitalTrendsSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_archivos_en_carpeta(ruta):\n",
        "    try:\n",
        "        # Lista todos los archivos en la carpeta\n",
        "        archivos = os.listdir(ruta)\n",
        "\n",
        "        # Filtra solo los archivos (no directorios)\n",
        "        archivos = [archivo for archivo in archivos if os.path.isfile(os.path.join(ruta, archivo))]\n",
        "\n",
        "        # Devuelve el número de archivos\n",
        "        return len(archivos)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al contar archivos: {e}\")\n",
        "        return None\n",
        "\n",
        "contar_archivos_en_carpeta(\"digitalTrends\")"
      ],
      "metadata": {
        "id": "Iqc7lll3MbL4",
        "outputId": "a9dbb636-9f03-42f8-808f-bee699ccdc73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2861"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parte 2 – Buscador (0,75 puntos)\n"
      ],
      "metadata": {
        "id": "exxHlc9_TnWH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fo8DaGgdTko5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
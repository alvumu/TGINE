{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGcgFs-tkxlS"
      },
      "source": [
        "# Sesión 3 - Scrapy - Ejemplo Crawling JSON-LD (ElMundo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJxLBFxilBOg"
      },
      "source": [
        "## Apartado 1.1 Crawler de El Mundo extrayendo JSON-LD\n",
        "En el ejemplo siguiente definimos un crawler en Scrapy para extraer noticias de los metadatos de las páginas web.\n",
        "El Mundo y otros periódicos publican metadatos en formato JSON-LD (https://json-ld.org/) que permite obtener información estructurada de las webs. En este caso, tendremos que obtener esos objetos JSON-LD y extraer su información en el formato de noticia (https://schema.org/NewsArticle) publicado en Schema.org\n",
        "\n",
        "** Existen librerías de Python para trabajar directamente con JSON-LD"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U scrapy"
      ],
      "metadata": {
        "id": "B2NYGvfRDtkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WEGeg3PzCZo"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class ElMundoSpider (scrapy.Spider):\n",
        "\n",
        "    #Es obligatorio poner un nombre\n",
        "    name = 'elmundo'\n",
        "\n",
        "    #Ponemos que el dominio que está permitido es el de la página y no vamos a irnos fuera de la misma\n",
        "    allowed_domains = ['www.elmundo.es']\n",
        "\n",
        "    start_urls = ['https://www.elmundo.es/']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    def parse (self, response):\n",
        "      \"\"\"\n",
        "      @inherit\n",
        "\n",
        "      @param self\n",
        "      @param response\n",
        "      \"\"\"\n",
        "\n",
        "      # Obtenemos la URL de la web que estamos procesando\n",
        "      url = str(response.request.url).strip()\n",
        "\n",
        "      # Obtenemos usando CSS la parte donde está definido el JSON-LD en la web\n",
        "      for item in response.css('script[type=\\\"application/ld+json\\\"]'):\n",
        "        data = json.loads(item.css('::text').get())\n",
        "        # Procesamos aquí los datos\n",
        "        # RELLENAR\n",
        "        title = \"\"\n",
        "        content = \"\"\n",
        "        date = \"\"\n",
        "        if url :\n",
        "\n",
        "            print (\"-------------------------\")\n",
        "            print (url)\n",
        "            print (title)\n",
        "            print (content)\n",
        "            print (date)\n",
        "            print (\"-------------------------\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'content': content,\n",
        "                'date': date\n",
        "            }\n",
        "\n",
        "            #Creamos para cada item un fichero json y para ello obtenemos un número aleatorio.\n",
        "            filename = str(random.random()).replace(\".\",\"\") + \".json\"\n",
        "\n",
        "            # Guardamos el documento si tiene contenido y título\n",
        "            if content and title:\n",
        "                with open ('elmundo/' + filename, 'w') as f:\n",
        "                    json.dump (data, f, indent = 4)\n",
        "\n",
        "      # Por último, miramos distintos enlaces que existan en la web (en este caso todos los de la etiqueta <a>)\n",
        "      url_in_current_document = response.css ('a')\n",
        "      for next_page in url_in_current_document:\n",
        "          # Por cada enlace ejecutamos el mismo parse\n",
        "          yield response.follow (next_page, self.parse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgv-fxRYl3wJ"
      },
      "source": [
        "## Apartado 1.2\n",
        "Para poder lanzar el Spider necesitamos que ejecutar el siguiente código donde se configuará y lanzará el proceso.\n",
        "Hay que hacer notar que solamente se puede lanzar un proceso por cada sesión en Jupyter notebook es por eso por lo que se recomienda exportar el código en un script de Python .py para poder ejecutarlo desde la línea de comandos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H27QM4rJCapR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (not os.path.exists('elmundo')):\n",
        "    os.mkdir('elmundo')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(ElMundoSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21fKpfTiDrhw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

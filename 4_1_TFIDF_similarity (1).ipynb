{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alvumu/BDGE/blob/main/4_1_TFIDF_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEdGLPXLdvCG"
      },
      "source": [
        "# Sesión 4.1 - Procesamiento de texto con la librería scikit-learn\n",
        "En esta sesión de prácticas vamos a ver cómo podemo usar las funciones de tratamiento de texto de la librería scikit-learn para calcular TF, TF-IDF y BM25\n",
        "\n",
        "Primero, instalaremos la librerías necesarias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "an8uq5WbeOZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d6e672-069c-48f3-f245-4874a71720a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.2\n",
            "--2023-11-02 17:06:42--  https://dis.um.es/~valencia/recursosTGINE/BM25.py\n",
            "Resolving dis.um.es (dis.um.es)... 155.54.239.5\n",
            "Connecting to dis.um.es (dis.um.es)|155.54.239.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3046 (3.0K) [text/x-python]\n",
            "Saving to: ‘BM25.py’\n",
            "\n",
            "BM25.py             100%[===================>]   2.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-02 17:06:43 (114 MB/s) - ‘BM25.py’ saved [3046/3046]\n",
            "\n",
            "--2023-11-02 17:06:43--  https://dis.um.es/~valencia/recursosTGINE/datasetEspa%C3%B1ol.csv\n",
            "Resolving dis.um.es (dis.um.es)... 155.54.239.5\n",
            "Connecting to dis.um.es (dis.um.es)|155.54.239.5|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1953117 (1.9M) [text/csv]\n",
            "Saving to: ‘datasetEspañol.csv’\n",
            "\n",
            "datasetEspañol.csv  100%[===================>]   1.86M  2.02MB/s    in 0.9s    \n",
            "\n",
            "2023-11-02 17:06:45 (2.02 MB/s) - ‘datasetEspañol.csv’ saved [1953117/1953117]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U scikit-learn\n",
        "# Descargamos un fichero python con la implementación del BM25\n",
        "!wget https://dis.um.es/~valencia/recursosTGINE/BM25.py\n",
        "\n",
        "# Descargamos el fichero datasetEspañol.csv\n",
        "!wget https://dis.um.es/~valencia/recursosTGINE/datasetEspañol.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n8Wt0FkzyW19"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MarKmlc0Cyaj"
      },
      "outputs": [],
      "source": [
        "# Importamos del fichero BM25.py\n",
        "from BM25 import BM25Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p1ldLK4Hye0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "bc244e33-e89d-4e54-c1b9-d0147b416f00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               twitter_id   twitter_created_at  \\\n",
              "5954  1274324047581581316  2020-06-20 16:51:43   \n",
              "5955  1274321386446733315  2020-06-20 16:41:08   \n",
              "5956  1274340519271858178  2020-06-20 17:57:10   \n",
              "5957  1274367246979211269  2020-06-20 19:43:22   \n",
              "5958  1274368625047220224  2020-06-20 19:48:51   \n",
              "\n",
              "                                                  tweet  \\\n",
              "5954  No me fío nada! ? en #estadodealarma demostrar...   \n",
              "5955  @roldanfj1 @MikiyDuarte @diariocadiz @realDona...   \n",
              "5956  Con el fin del #EstadodeAlarma se acaban, tamb...   \n",
              "5957  @horaciorlarreta. @AsisOberdan. @luisnovaresio...   \n",
              "5958  En la última semana se han registrado 36 falle...   \n",
              "\n",
              "                                   corpus             user  agreement  votes  \\\n",
              "5954  Estado de alarma nacional (oficial)   GuzmanitaMaria        100      1   \n",
              "5955  Estado de alarma nacional (oficial)        ByChanchi        100      1   \n",
              "5956  Estado de alarma nacional (oficial)  Javiersilvestre        100      1   \n",
              "5957  Estado de alarma nacional (oficial)     juliodebarna        100      1   \n",
              "5958  Estado de alarma nacional (oficial)   mallorcadiario        100      1   \n",
              "\n",
              "      score     label __split  \n",
              "5954     -1  negative    test  \n",
              "5955     -1  negative   train  \n",
              "5956     -1  negative     val  \n",
              "5957     -1  negative   train  \n",
              "5958     -1  negative   train  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aaaab047-6279-47df-82e9-91f2fa62fdfc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>twitter_id</th>\n",
              "      <th>twitter_created_at</th>\n",
              "      <th>tweet</th>\n",
              "      <th>corpus</th>\n",
              "      <th>user</th>\n",
              "      <th>agreement</th>\n",
              "      <th>votes</th>\n",
              "      <th>score</th>\n",
              "      <th>label</th>\n",
              "      <th>__split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5954</th>\n",
              "      <td>1274324047581581316</td>\n",
              "      <td>2020-06-20 16:51:43</td>\n",
              "      <td>No me fío nada! ? en #estadodealarma demostrar...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>GuzmanitaMaria</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>1274321386446733315</td>\n",
              "      <td>2020-06-20 16:41:08</td>\n",
              "      <td>@roldanfj1 @MikiyDuarte @diariocadiz @realDona...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>ByChanchi</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5956</th>\n",
              "      <td>1274340519271858178</td>\n",
              "      <td>2020-06-20 17:57:10</td>\n",
              "      <td>Con el fin del #EstadodeAlarma se acaban, tamb...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>Javiersilvestre</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>val</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5957</th>\n",
              "      <td>1274367246979211269</td>\n",
              "      <td>2020-06-20 19:43:22</td>\n",
              "      <td>@horaciorlarreta. @AsisOberdan. @luisnovaresio...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>juliodebarna</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5958</th>\n",
              "      <td>1274368625047220224</td>\n",
              "      <td>2020-06-20 19:48:51</td>\n",
              "      <td>En la última semana se han registrado 36 falle...</td>\n",
              "      <td>Estado de alarma nacional (oficial)</td>\n",
              "      <td>mallorcadiario</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>negative</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aaaab047-6279-47df-82e9-91f2fa62fdfc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aaaab047-6279-47df-82e9-91f2fa62fdfc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aaaab047-6279-47df-82e9-91f2fa62fdfc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4dc87ad8-77a3-432c-ba82-74cc16bd2f9d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4dc87ad8-77a3-432c-ba82-74cc16bd2f9d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4dc87ad8-77a3-432c-ba82-74cc16bd2f9d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Leemos los datos del dataset en español de la primera sesión\n",
        "data = pd.read_csv(\"datasetEspañol.csv\",encoding=\"UTF-8\")\n",
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPOoOqEf5NG"
      },
      "source": [
        "## Apartado 1.1 - Obtener TF del conjunto de texto (Resuelto)\n",
        "\n",
        "Calculamos la matriz de TF usando la clase CountVectorizer sobre un conjunto de textos.\n",
        "\n",
        "Se puede consultar información de esta clase en la siguiente URL:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
        "\n",
        "Hay que tener en cuenta que esta clase tiene muchos parámetros en su método de creación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g1axNe99hQG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d966e3-cf87-4fad-928d-512e9b98d8a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 36)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "  (0, 16)\t3\n",
            "  (0, 1)\t3\n",
            "  (0, 4)\t6\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 32)\t2\n",
            "  (0, 5)\t2\n",
            "  (0, 21)\t2\n",
            "  (0, 2)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 29)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 7)\t2\n",
            "  (0, 33)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 25)\t2\n",
            "  (0, 18)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 30)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 9)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "  (0, 24)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 23)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 11)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "  (0, 16)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 34)\t1\n",
            "  (0, 12)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([('la', 16), ('asignatura', 1), ('de', 4), ('tgine', 31), ('es', 8), ('una', 32), ('del', 5), ('máster', 21), ('bigdata', 2), ('que', 27), ('se', 29), ('estudia', 10), ('en', 7), ('universidad', 33), ('murcia', 20), ('vemos', 35), ('introducción', 15), ('al', 0), ('procesamiento', 25), ('lenguaje', 18), ('natural', 22), ('tecnologías', 30), ('información', 14), ('no', 24), ('estructurada', 9), ('me', 19), ('gusta', 13), ('el', 6), ('chocolate', 3), ('ni', 23), ('las', 17), ('fresas', 11), ('profesor', 26), ('rafael', 28), ('valencia', 34), ('garcía', 12)])\n",
            "Tamaño vocabulario: 36\n",
            "Código de la palabra TGINE es: 31\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
        "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
        "de información no estructurada.\n",
        "\"\"\"\n",
        "texto2 = \"No me gusta el chocolate ni las fresas\"\n",
        "\n",
        "texto3 = \"\"\"El profesor de la asignatura TGINE es Rafael Valencia García.\n",
        "\"\"\"\n",
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform([texto,texto2,texto3])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())\n",
        "print(\"Tamaño vocabulario:\", str(len(count_vect.vocabulary_.items())))\n",
        "\n",
        "#Mostramos el código de una palabra determinada\n",
        "#hay que tener en cuenta que todos los tokens se guardan en minúsculas\n",
        "palabra_a_buscar=\"TGINE\"\n",
        "print(\"Código de la palabra\", palabra_a_buscar, \"es:\", count_vect.vocabulary_.get(palabra_a_buscar.lower()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hMjf7pTYDl"
      },
      "source": [
        "## Apartado 1.2 Calculamos el TF sin tener en cuenta las stopwords\n",
        "Para eso hacemos uso del parámetro stop_words de CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2xBK8v0wTYDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1ae3d7-f3ca-44cc-c546-7962b41c64cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 22)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "  (0, 0)\t3\n",
            "  (0, 18)\t1\n",
            "  (0, 12)\t2\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 21)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 14)\t2\n",
            "  (0, 10)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 3)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "  (0, 7)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "  (0, 0)\t1\n",
            "  (0, 18)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 16)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 6)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([('asignatura', 0), ('tgine', 18), ('máster', 12), ('bigdata', 1), ('estudia', 4), ('universidad', 19), ('murcia', 11), ('vemos', 21), ('introducción', 9), ('procesamiento', 14), ('lenguaje', 10), ('natural', 13), ('tecnologías', 17), ('información', 8), ('estructurada', 3), ('gusta', 7), ('chocolate', 2), ('fresas', 5), ('profesor', 15), ('rafael', 16), ('valencia', 20), ('garcía', 6)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Descargamos las stopwords de NLTK\n",
        "# Si no tenemos instalado NLTK lo instalamos\n",
        "# !pip3 install -U nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('spanish')\n",
        "\n",
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer(stop_words=stopwords)\n",
        "X_counts = count_vect.fit_transform([texto,texto2,texto3])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn7dSDhxjE4R"
      },
      "source": [
        "## Apartado 1.3 TF-IDF (Resuelto)\n",
        "Obtenemos el TF-IDF utilizando la clase TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "03E_i6avjAiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e03074-6eab-463e-e8a6-0a2648bb697b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5959, 23922)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "  (0, 23725)\t0.049423969305183627\n",
            "  (0, 22606)\t0.17275793273390508\n",
            "  (0, 22067)\t0.1503006371554885\n",
            "  (0, 21279)\t0.24015722738486325\n",
            "  (0, 21278)\t0.24015722738486325\n",
            "  (0, 20860)\t0.2157059272965902\n",
            "  (0, 14830)\t0.24015722738486325\n",
            "  (0, 14082)\t0.24015722738486325\n",
            "  (0, 13452)\t0.21084066025562398\n",
            "  (0, 13388)\t0.24015722738486325\n",
            "  (0, 13387)\t0.24015722738486325\n",
            "  (0, 11621)\t0.09324696322101765\n",
            "  (0, 10888)\t0.4803144547697265\n",
            "  (0, 6843)\t0.2157059272965902\n",
            "  (0, 6839)\t0.24015722738486325\n",
            "  (0, 5362)\t0.24015722738486325\n",
            "  (0, 4063)\t0.24015722738486325\n",
            "  (0, 1018)\t0.22166053291045124\n",
            "No me gusta el chocolate ni las fresas\n",
            "  (0, 23863)\t0.26673983724063893\n",
            "  (0, 23725)\t0.078522438570141\n",
            "  (0, 22775)\t0.3521636530528547\n",
            "  (0, 22219)\t0.2267770326850309\n",
            "  (0, 20257)\t0.3427032604519152\n",
            "  (0, 20184)\t0.22209050007180056\n",
            "  (0, 18110)\t0.2450828773016924\n",
            "  (0, 11621)\t0.14814631532653166\n",
            "  (0, 7436)\t0.36436022039086163\n",
            "  (0, 5980)\t0.3427032604519152\n",
            "  (0, 5850)\t0.24424332523913628\n",
            "  (0, 5661)\t0.3427032604519152\n",
            "  (0, 5191)\t0.26673983724063893\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "  (0, 23725)\t0.050533454000939226\n",
            "  (0, 23024)\t0.12633179213045934\n",
            "  (0, 21774)\t0.19903579243738106\n",
            "  (0, 21624)\t0.20451091669799148\n",
            "  (0, 21537)\t0.16059881214271426\n",
            "  (0, 20560)\t0.20772452371981118\n",
            "  (0, 18575)\t0.15512368788210384\n",
            "  (0, 18415)\t0.15229867109565434\n",
            "  (0, 16345)\t0.21136781087555595\n",
            "  (0, 15997)\t0.1345285188100867\n",
            "  (0, 13658)\t0.21136781087555595\n",
            "  (0, 13619)\t0.18412382584789877\n",
            "  (0, 13472)\t0.1777498491374627\n",
            "  (0, 12948)\t0.18559900265036572\n",
            "  (0, 11259)\t0.11780049997276576\n",
            "  (0, 11238)\t0.17556595392715788\n",
            "  (0, 9378)\t0.22663643776743694\n",
            "  (0, 9023)\t0.21136781087555595\n",
            "  (0, 7878)\t0.2016362472597855\n",
            "  (0, 7486)\t0.21136781087555595\n",
            "  (0, 6900)\t0.16824660588847212\n",
            "  (0, 5927)\t0.15320822614029425\n",
            "  (0, 5226)\t0.2455483518150627\n",
            "  (0, 4903)\t0.20772452371981118\n",
            "  (0, 4682)\t0.18881260967218544\n",
            "  (0, 4174)\t0.19727423511753553\n",
            "  (0, 3213)\t0.2455483518150627\n",
            "  (0, 876)\t0.2455483518150627\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_TFIDF.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_TFIDF[0])\n",
        "print(texto2)\n",
        "print(X_TFIDF[1])\n",
        "print(texto3)\n",
        "print(X_TFIDF[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liEFSQ-rpfBd"
      },
      "source": [
        "## Apartado 1.4 BM25 (Resuelto)\n",
        "\n",
        "Calculamos el BM25 que tiene en cuenta tanto la transformación del TF como la normalización de la longitud del documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fCDDgaBwpepN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3515b839-ac26-4473-e43c-94f96d58196c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5959, 23922)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "  (0, 23725)\t0.052549796023945355\n",
            "  (0, 22606)\t0.1836840353842854\n",
            "  (0, 22067)\t0.1598064246119052\n",
            "  (0, 21279)\t0.25534600903507787\n",
            "  (0, 21278)\t0.25534600903507787\n",
            "  (0, 20860)\t0.2293482826237298\n",
            "  (0, 14830)\t0.25534600903507787\n",
            "  (0, 14082)\t0.25534600903507787\n",
            "  (0, 13452)\t0.22417531100289353\n",
            "  (0, 13388)\t0.25534600903507787\n",
            "  (0, 13387)\t0.25534600903507787\n",
            "  (0, 11621)\t0.09914438208836628\n",
            "  (0, 10888)\t0.36099317521931923\n",
            "  (0, 6843)\t0.2293482826237298\n",
            "  (0, 6839)\t0.25534600903507787\n",
            "  (0, 5362)\t0.25534600903507787\n",
            "  (0, 4063)\t0.25534600903507787\n",
            "  (0, 1018)\t0.23567948820698148\n",
            "No me gusta el chocolate ni las fresas\n",
            "  (0, 23863)\t0.2667398372406389\n",
            "  (0, 23725)\t0.078522438570141\n",
            "  (0, 22775)\t0.35216365305285463\n",
            "  (0, 22219)\t0.22677703268503088\n",
            "  (0, 20257)\t0.3427032604519152\n",
            "  (0, 20184)\t0.22209050007180056\n",
            "  (0, 18110)\t0.24508287730169237\n",
            "  (0, 11621)\t0.14814631532653166\n",
            "  (0, 7436)\t0.36436022039086163\n",
            "  (0, 5980)\t0.3427032604519152\n",
            "  (0, 5850)\t0.24424332523913628\n",
            "  (0, 5661)\t0.3427032604519152\n",
            "  (0, 5191)\t0.2667398372406389\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "  (0, 23725)\t0.05096956071454195\n",
            "  (0, 23024)\t0.12742204301037216\n",
            "  (0, 21774)\t0.20075348316415323\n",
            "  (0, 21624)\t0.20627585807277626\n",
            "  (0, 21537)\t0.16198478944342992\n",
            "  (0, 20560)\t0.20951719871432975\n",
            "  (0, 18575)\t0.15646241453480686\n",
            "  (0, 18415)\t0.15361301768546698\n",
            "  (0, 16345)\t0.21319192765492012\n",
            "  (0, 15997)\t0.1356895079287608\n",
            "  (0, 13658)\t0.21319192765492012\n",
            "  (0, 13619)\t0.18571282541608572\n",
            "  (0, 13472)\t0.17928384090753402\n",
            "  (0, 12948)\t0.18720073308209698\n",
            "  (0, 11259)\t0.11881712529394262\n",
            "  (0, 11238)\t0.1770810985516729\n",
            "  (0, 9378)\t0.2285923237050091\n",
            "  (0, 9023)\t0.21319192765492012\n",
            "  (0, 7878)\t0.20337638006634204\n",
            "  (0, 7486)\t0.21319192765492012\n",
            "  (0, 6900)\t0.16969858410408095\n",
            "  (0, 5927)\t0.15453042224423966\n",
            "  (0, 5226)\t0.24766744869568838\n",
            "  (0, 4903)\t0.20951719871432975\n",
            "  (0, 4682)\t0.19044207372365043\n",
            "  (0, 4174)\t0.14918826902377377\n",
            "  (0, 3213)\t0.24766744869568838\n",
            "  (0, 876)\t0.24766744869568838\n"
          ]
        }
      ],
      "source": [
        "bm25_transformer = BM25Transformer()\n",
        "X_BM25 = bm25_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_BM25.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_BM25[0])\n",
        "print(texto2)\n",
        "print(X_BM25[1])\n",
        "print(texto3)\n",
        "print(X_BM25[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsOfKhgeaCw"
      },
      "source": [
        "# Apartado 1.5 Desarrollamos un simple buscador con TFIDF y BM25 (Resuelto)\n",
        "\n",
        "A continuación vamos a procesar los tuits del archivo de la primera sesión \"datosEspañol.csv\" y calculamos el TF, el TFIDF y el BM25 de manera similar a como se ha hecho anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Sxt-6GdmzVlA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dbe905-e5d8-404c-90f4-56b9a82704f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5959, 75058)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "  (0, 3919)\t1\n",
            "  (0, 37525)\t1\n",
            "  (0, 53991)\t1\n",
            "  (0, 68896)\t2\n",
            "  (0, 3970)\t1\n",
            "  (0, 37673)\t1\n",
            "  (0, 53992)\t1\n",
            "  (0, 3980)\t1\n",
            "  (0, 37674)\t1\n",
            "  (0, 4410)\t2\n",
            "  (0, 46100)\t4\n",
            "  (0, 32964)\t2\n",
            "  (0, 56882)\t3\n",
            "  (0, 32330)\t5\n",
            "  (0, 48129)\t3\n",
            "  (0, 29487)\t3\n",
            "  (0, 51933)\t1\n",
            "  (0, 43048)\t3\n",
            "  (0, 19847)\t4\n",
            "  (0, 4444)\t1\n",
            "  (0, 46246)\t1\n",
            "  (0, 33084)\t1\n",
            "  (0, 57077)\t1\n",
            "  (0, 32423)\t1\n",
            "  (0, 48233)\t1\n",
            "  :\t:\n",
            "  (0, 35873)\t1\n",
            "  (0, 56007)\t1\n",
            "  (0, 48716)\t1\n",
            "  (0, 35963)\t1\n",
            "  (0, 32965)\t1\n",
            "  (0, 52455)\t1\n",
            "  (0, 48727)\t1\n",
            "  (0, 35964)\t1\n",
            "  (0, 61203)\t1\n",
            "  (0, 46373)\t1\n",
            "  (0, 39409)\t1\n",
            "  (0, 34541)\t1\n",
            "  (0, 73508)\t1\n",
            "  (0, 9191)\t1\n",
            "  (0, 61241)\t1\n",
            "  (0, 46462)\t1\n",
            "  (0, 39479)\t1\n",
            "  (0, 43924)\t1\n",
            "  (0, 34542)\t1\n",
            "  (0, 565)\t1\n",
            "  (0, 9193)\t1\n",
            "  (0, 61247)\t1\n",
            "  (0, 46469)\t1\n",
            "  (0, 39489)\t1\n",
            "  (0, 43925)\t1\n"
          ]
        }
      ],
      "source": [
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer(stop_words=stopwords, analyzer = \"char_wb\", ngram_range=(2,4))\n",
        "X_counts = count_vect.fit_transform(data['tweet'])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_counts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CC1CNRQlATwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b869a939-c8e9-4c14-8b4d-0a8842f836c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45658"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# El vocabulario que forma los tokens del objeto vectorizer se puede obtener de la siguiente manera\n",
        "# obtenemos el id del token 'buenosdias' que proviene de un hashtag\n",
        "count_vect.vocabulary_.get('ma')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R9in80glzWGo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809c60ec-b08c-41e2-f580-9e2f37425115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5959, 23922)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "  (0, 23725)\t0.049423969305183627\n",
            "  (0, 22606)\t0.17275793273390508\n",
            "  (0, 22067)\t0.1503006371554885\n",
            "  (0, 21279)\t0.24015722738486325\n",
            "  (0, 21278)\t0.24015722738486325\n",
            "  (0, 20860)\t0.2157059272965902\n",
            "  (0, 14830)\t0.24015722738486325\n",
            "  (0, 14082)\t0.24015722738486325\n",
            "  (0, 13452)\t0.21084066025562398\n",
            "  (0, 13388)\t0.24015722738486325\n",
            "  (0, 13387)\t0.24015722738486325\n",
            "  (0, 11621)\t0.09324696322101765\n",
            "  (0, 10888)\t0.4803144547697265\n",
            "  (0, 6843)\t0.2157059272965902\n",
            "  (0, 6839)\t0.24015722738486325\n",
            "  (0, 5362)\t0.24015722738486325\n",
            "  (0, 4063)\t0.24015722738486325\n",
            "  (0, 1018)\t0.22166053291045124\n",
            "(5959, 23922)\n",
            "Hoy merendola deliciosa! Latte Macchiato Caramelo con Leche Condensada y Gofre! ???. #yomequedoencasa #todovaasalirbien #undiamenos #actitudpositiva #lattemacchiato #gofre #delicious #strong #stronger #smile…\n",
            "  (0, 23725)\t0.052549796023945355\n",
            "  (0, 22606)\t0.1836840353842854\n",
            "  (0, 22067)\t0.1598064246119052\n",
            "  (0, 21279)\t0.25534600903507787\n",
            "  (0, 21278)\t0.25534600903507787\n",
            "  (0, 20860)\t0.2293482826237298\n",
            "  (0, 14830)\t0.25534600903507787\n",
            "  (0, 14082)\t0.25534600903507787\n",
            "  (0, 13452)\t0.22417531100289353\n",
            "  (0, 13388)\t0.25534600903507787\n",
            "  (0, 13387)\t0.25534600903507787\n",
            "  (0, 11621)\t0.09914438208836628\n",
            "  (0, 10888)\t0.36099317521931923\n",
            "  (0, 6843)\t0.2293482826237298\n",
            "  (0, 6839)\t0.25534600903507787\n",
            "  (0, 5362)\t0.25534600903507787\n",
            "  (0, 4063)\t0.25534600903507787\n",
            "  (0, 1018)\t0.23567948820698148\n"
          ]
        }
      ],
      "source": [
        "# Calculamos ahora el TFIDF\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_TFIDF.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_TFIDF[0])\n",
        "\n",
        "# Calculamos también el BM25\n",
        "X_BM25 = bm25_transformer.fit_transform(X_counts)\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_BM25.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos el primer tuit y su correspondientes TF\n",
        "print(data['tweet'][0])\n",
        "print(X_BM25[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0b40Vpc8zjAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f195409b-36f9-45b2-8bd4-52eee4f2c899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5959, 23922)\n",
            "(5959, 23922)\n"
          ]
        }
      ],
      "source": [
        "# Podemos ver que el tamaño de la matriz de textos y el número total de tokens es el mismo tanto\n",
        "# para TFIDF como para BM25\n",
        "print(X_TFIDF.shape) # (Number of tweets, Number of unique words)\n",
        "print(X_BM25.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNqweJpqfpcS"
      },
      "source": [
        "Realizamos una consulta cualquiera y la metemos en el string \"query\" para a continuación calcular la similitud del coseno usando el TF-IDF y el BM25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "qQg-FiyLz1mZ"
      },
      "outputs": [],
      "source": [
        "query = \"semana santa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eS-z3SCdz7y6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "e255f4cf-e39f-48b3-cbe5-bbe5d80c363c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-d51d4fbd0db4>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery_vec_TFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_TFIDF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_vec_TFIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Op -- (n_docs,1) -- Cosine Sim with each doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;34m\"Incompatible dimension for X and Y matrices: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;34m\"X.shape[1] == %d while Y.shape[1] == %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 23922 while Y.shape[1] == 75058"
          ]
        }
      ],
      "source": [
        "# Transformamos la query a TF-ID y sacamos los resultados de la comparación con la función del coseno\n",
        "# cosine_similarity\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_TFIDF = tfidf_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_TFIDF,query_vec_TFIDF).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rOKu2It3FRq9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "08db626e-f603-43c2-dac9-807654801590"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-201ae9931889>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquery_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquery_vec_BM25\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_BM25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery_vec_BM25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Op -- (n_docs,1) -- Cosine Sim with each doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0;34m\"Incompatible dimension for X and Y matrices: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;34m\"X.shape[1] == %d while Y.shape[1] == %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 23922 while Y.shape[1] == 75058"
          ]
        }
      ],
      "source": [
        "# Obtenemos ahora los resultados usando el BM25\n",
        "query_vec = count_vect.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
        "query_vec_BM25 = bm25_transformer.fit_transform(query_vec)\n",
        "results = cosine_similarity(X_BM25,query_vec_BM25).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "# Immprimimos a continuación los primeros 10 resultados ordenados por la similitud obtenida\n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(data.iloc[i,2],\"--\",results[i],\"--\",i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5kye10CTYDn"
      },
      "source": [
        "## Apartado 1.5 Calculamos la similitud de varios textos con TF-IDF y BM25 (Resuelto)\n",
        "\n",
        "Vamos a calcular la similitud de el primer texto con respecto a los demás usando la similitud del coseno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9sBC9HP8TYDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a861f834-07bc-4bbf-f1ca-c752515a18c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.82218193 0.16800859 0.03486424 0.15382285]\n",
            "[0.7465174  0.1372938  0.03771061 0.1419987 ]\n"
          ]
        }
      ],
      "source": [
        "# Definimos un conjunto de textos\n",
        "textos=['El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. \\\n",
        "Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje.\\\n",
        "Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'El procesamiento del lenguaje natural (NLP, por sus siglas en inglés) es una rama de la inteligencia artificial que ayuda a las computadoras a entender, interpretar y manipular el lenguaje humano. \\\n",
        "NLP toma elementos prestados de muchas disciplinas, incluyendo la ciencia de la computación y la lingüística computacional, en su afán por cerrar la brecha entre la comunicación humana y el entendimiento de las computadoras.\"\"\"], \"\"\"El procesamiento del lenguaje natural (PLN o NLP) es un campo dentro de la inteligencia artificial y la lingüística aplicada que estudia las interacciones mediante uso del lenguaje natural entre los seres humanos y las máquinas. Más concretamente se centra en el procesamiento de las comunicaciones humanas, dividiéndolas en partes, e identificando los elementos más relevantes del mensaje. Con la Comprensión y Generación de Lenguaje Natural, busca que las máquinas consigan entender, interpretar y manipular el lenguaje humano.'\n",
        ", 'La lingüística computacional es un campo interdisciplinario que se ocupa del desarrollo de formalismos del funcionamiento del lenguaje natural, tales que puedan ser transformados en programas ejecutables para un ordenador. \\\n",
        "Dicho desarrollo se sitúa entre el modelado basado en reglas y el modelado estadístico del lenguaje natural desde una perspectiva computacional, y en él participan lingüistas e informáticos especializados en inteligencia artificial, psicólogos cognoscitivos y expertos en lógica, entre otros.'\n",
        ", 'El aprendizaje automático es un tipo de inteligencia artificial (AI) que proporciona a las computadoras la capacidad de aprender, sin ser programadas explícitamente. El aprendizaje automático se centra en el desarrollo de programas informáticos que pueden cambiar cuando se exponen a nuevos datos.'\n",
        ", 'El  aprendizaje profundo es un tema que cada vez adquiere mayor relevancia en el campo de la inteligencia artificial (IA). Siendo una subcategoría del aprendizaje automático, el aprendizaje profundo trata del uso de redes neuronales para mejorar cosas tales como el reconocimiento de voz, la visión por ordenador y el procesamiento del lenguaje natural. \\\n",
        "Rápidamente se está convirtiendo en uno de los campos más solicitados en informática. \\\n",
        "En los últimos años, el aprendizaje profundo ha ayudado a lograr avances en áreas tan diversas como la percepción de objetos, el procesamiento del lenguaje natural y el reconocimiento de voz (todas ellas áreas especialmente complejas para los investigadores en IA).'\n",
        "]\n",
        "\n",
        "# Calculamos la similitud usando TFIDF\n",
        "count_vect = CountVectorizer(stop_words=stopwords)\n",
        "X_counts = count_vect.fit_transform(textos)\n",
        "\n",
        "# Calculamos ahora el TFIDF\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_TFIDF = tfidf_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Calculamos también el BM25\n",
        "BM25_transformer = BM25Transformer(k=1.2,b=0.75)\n",
        "X_BM25 = BM25_transformer.fit_transform(X_counts)\n",
        "\n",
        "# Calculamos la similitud de los documentos con el coseno para TFIDF\n",
        "results = cosine_similarity(X_TFIDF[1::],X_TFIDF[0]).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(results)\n",
        "\n",
        "# Calculamos la similitud de los textos con el coseno para BM25\n",
        "results = cosine_similarity(X_BM25[1::],X_TFIDF[0]).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.6 Cambiando a character n-grams\n",
        "Podemos usar en vez de palabras (words), character n-grams para crear el vocabulario. Lo vemos con el primer ejemplo"
      ],
      "metadata": {
        "id": "oxdP7JACcBqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KtOJF8z9TYDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6adeac7-17a5-479f-a0f6-64fe22009061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 36)\n",
            "La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
            "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
            "de información no estructurada.\n",
            "\n",
            "  (0, 16)\t3\n",
            "  (0, 1)\t3\n",
            "  (0, 4)\t6\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 32)\t2\n",
            "  (0, 5)\t2\n",
            "  (0, 21)\t2\n",
            "  (0, 2)\t1\n",
            "  (0, 27)\t1\n",
            "  (0, 29)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 7)\t2\n",
            "  (0, 33)\t1\n",
            "  (0, 20)\t1\n",
            "  (0, 35)\t1\n",
            "  (0, 15)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 25)\t2\n",
            "  (0, 18)\t1\n",
            "  (0, 22)\t1\n",
            "  (0, 30)\t1\n",
            "  (0, 14)\t1\n",
            "  (0, 24)\t1\n",
            "  (0, 9)\t1\n",
            "No me gusta el chocolate ni las fresas\n",
            "  (0, 24)\t1\n",
            "  (0, 19)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 23)\t1\n",
            "  (0, 17)\t1\n",
            "  (0, 11)\t1\n",
            "El profesor de la asignatura TGINE es Rafael Valencia García.\n",
            "\n",
            "  (0, 16)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 31)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 26)\t1\n",
            "  (0, 28)\t1\n",
            "  (0, 34)\t1\n",
            "  (0, 12)\t1\n",
            "Mostramos los items del diccionario\n",
            "dict_items([('la', 16), ('asignatura', 1), ('de', 4), ('tgine', 31), ('es', 8), ('una', 32), ('del', 5), ('máster', 21), ('bigdata', 2), ('que', 27), ('se', 29), ('estudia', 10), ('en', 7), ('universidad', 33), ('murcia', 20), ('vemos', 35), ('introducción', 15), ('al', 0), ('procesamiento', 25), ('lenguaje', 18), ('natural', 22), ('tecnologías', 30), ('información', 14), ('no', 24), ('estructurada', 9), ('me', 19), ('gusta', 13), ('el', 6), ('chocolate', 3), ('ni', 23), ('las', 17), ('fresas', 11), ('profesor', 26), ('rafael', 28), ('valencia', 34), ('garcía', 12)])\n",
            "Tamaño vocabulario: 36\n",
            "Código de la palabra TGINE es: 31\n"
          ]
        }
      ],
      "source": [
        "texto = \"\"\"La asignatura de TGINE es una asignatura del máster de BigData que se estudia en la Universidad de Murcia.\n",
        "En la asignatura de máster vemos una introducción al procesamiento del lenguaje natural y tecnologías de procesamiento\n",
        "de información no estructurada.\n",
        "\"\"\"\n",
        "texto2 = \"No me gusta el chocolate ni las fresas\"\n",
        "\n",
        "texto3 = \"\"\"El profesor de la asignatura TGINE es Rafael Valencia García.\n",
        "\"\"\"\n",
        "# Calculamos la matriz de TF usando la función fit_transform\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform([texto,texto2,texto3])\n",
        "\n",
        "# Mostramos entonces el número de textos y el número de tokens únicos\n",
        "print(X_counts.shape)\n",
        "\n",
        "# X_counts es una matriz dispersa con el TF de cada token en cada texto\n",
        "# Imprimimos los textos y su correspondientes TF\n",
        "print(texto)\n",
        "print(X_counts[0])\n",
        "print(texto2)\n",
        "print(X_counts[1])\n",
        "print(texto3)\n",
        "print(X_counts[2])\n",
        "\n",
        "#Los tokens de todo el vocabulario se representan con ids que hacen referencia a cada token.\n",
        "print(\"Mostramos los items del diccionario\")\n",
        "print(count_vect.vocabulary_.items())\n",
        "print(\"Tamaño vocabulario:\", str(len(count_vect.vocabulary_.items())))\n",
        "\n",
        "#Mostramos el código de una palabra determinada\n",
        "#hay que tener en cuenta que todos los tokens se guardan en minúsculas\n",
        "palabra_a_buscar=\"TGINE\"\n",
        "print(\"Código de la palabra\", palabra_a_buscar, \"es:\", count_vect.vocabulary_.get(palabra_a_buscar.lower()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}